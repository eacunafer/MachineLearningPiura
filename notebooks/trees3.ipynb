{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import sys\n",
    "import functools \n",
    "import operator\n",
    "import itertools\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_index(sample_name):\n",
    "    '''\n",
    "    When the training data is read from a CSV file, we assume that the first column\n",
    "    of each data record contains a unique integer identifier for the record in that\n",
    "    row. This training data is stored in a dictionary whose keys are the prefix\n",
    "    'sample_' followed by the identifying integers. The purpose of this function is to\n",
    "    return the identifying integer associated with a data record.\n",
    "    '''\n",
    "    m = re.search('_(.+)$', sample_name)\n",
    "    return int(m.group(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d9f385e64160>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msample_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"stage3cancer.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-d93d4f520650>\u001b[0m in \u001b[0;36msample_index\u001b[1;34m(sample_name)\u001b[0m\n\u001b[0;32m      8\u001b[0m     '''\n\u001b[0;32m      9\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_(.+)$'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "sample_index(\"stage3cancer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    def __init__(self, *args, **kwargs ):\n",
    "        if kwargs and args:\n",
    "            raise SyntaxError(  \n",
    "                   '''DecisionTree constructor can only be called with keyword arguments for the \n",
    "                      following keywords: training_datafile,entropy_threshold, max_depth_desired, \n",
    "                      csv_class_column_index,symbolic_to_numeric_cardinality_threshold, number_of_histogram_bins,\n",
    "                      csv_columns_for_features,csv_cleanup_needed,number_of_histogram_bins, debug1, debug2, and debug3''') \n",
    "        allowed_keys = 'training_datafile','entropy_threshold','max_depth_desired','csv_class_column_index',\\\n",
    "                       'symbolic_to_numeric_cardinality_threshold','csv_columns_for_features',\\\n",
    "                       'number_of_histogram_bins','csv_cleanup_needed','debug1','debug2','debug3'\n",
    "        keywords_used = kwargs.keys()\n",
    "        for keyword in keywords_used:\n",
    "            if keyword not in allowed_keys:\n",
    "                raise SyntaxError(keyword + \":  Wrong keyword used --- check spelling\") \n",
    "        training_datafile=entropy_threshold=max_depth_desired=csv_class_column_index=number_of_histogram_bins= None\n",
    "        symbolic_to_numeric_cardinality_threshold=csv_columns_for_features=csv_cleanup_needed=debug1=debug2=debug3=None\n",
    "        if kwargs and not args:\n",
    "            if 'training_datafile' in kwargs : training_datafile = kwargs.pop('training_datafile')\n",
    "            if 'entropy_threshold' in kwargs : entropy_threshold = kwargs.pop('entropy_threshold')\n",
    "            if 'max_depth_desired' in kwargs : max_depth_desired = kwargs.pop('max_depth_desired')\n",
    "            if 'csv_class_column_index' in kwargs: csv_class_column_index = kwargs.pop('csv_class_column_index')\n",
    "            if 'csv_columns_for_features' in kwargs: \\\n",
    "                                  csv_columns_for_features = kwargs.pop('csv_columns_for_features')\n",
    "            if 'symbolic_to_numeric_cardinality_threshold' in kwargs: \\\n",
    "                           symbolic_to_numeric_cardinality_threshold = \\\n",
    "                                                         kwargs.pop('symbolic_to_numeric_cardinality_threshold')\n",
    "            if 'number_of_histogram_bins' in kwargs: \\\n",
    "                                               number_of_histogram_bins = kwargs.pop('number_of_histogram_bins')\n",
    "            if 'csv_cleanup_needed' in kwargs: csv_cleanup_needed = kwargs.pop('csv_cleanup_needed')\n",
    "            if 'debug1' in kwargs  :  debug1 = kwargs.pop('debug1')\n",
    "            if 'debug2' in kwargs  :  debug2 = kwargs.pop('debug2')\n",
    "            if 'debug3' in kwargs  :  debug3 = kwargs.pop('debug3')\n",
    "        if not args and training_datafile:\n",
    "            self._training_datafile = training_datafile\n",
    "        elif not args and not training_datafile:\n",
    "            raise SyntaxError('''You must specify a training datafile''')\n",
    "        else:\n",
    "            if (args[0] != 'evalmode') and (args[0] != 'boostingmode'):\n",
    "                raise SyntaxError(\"\"\"When supplying non-keyword arg, it can only be 'evalmode' or 'boostingmode'\"\"\")\n",
    "        if entropy_threshold: \n",
    "            self._entropy_threshold                         =      entropy_threshold\n",
    "        else:\n",
    "            self._entropy_threshold                         =      0.01        \n",
    "        if max_depth_desired:\n",
    "            self._max_depth_desired                         =      max_depth_desired \n",
    "        else:\n",
    "            self._max_depth_desired                         =      None\n",
    "        if number_of_histogram_bins:\n",
    "            self._number_of_histogram_bins                  =      number_of_histogram_bins\n",
    "        else:\n",
    "            self._number_of_histogram_bins                  =      None\n",
    "        if csv_class_column_index:\n",
    "            self._csv_class_column_index                    =      csv_class_column_index\n",
    "        else:\n",
    "            self._csv_class_column_index                    =      None\n",
    "        if csv_columns_for_features:\n",
    "            self._csv_columns_for_features                  =      csv_columns_for_features\n",
    "        else: \n",
    "            self._csv_columns_for_features                  =      None            \n",
    "        if symbolic_to_numeric_cardinality_threshold:\n",
    "            self._symbolic_to_numeric_cardinality_threshold =      symbolic_to_numeric_cardinality_threshold\n",
    "        else:\n",
    "            self._symbolic_to_numeric_cardinality_threshold =      10\n",
    "        if csv_cleanup_needed:\n",
    "            self._csv_cleanup_needed                        =      csv_cleanup_needed\n",
    "        else:\n",
    "            self._csv_cleanup_needed                        =      0\n",
    "        if debug1:\n",
    "            self._debug1                                    =      debug1\n",
    "        else:\n",
    "            self._debug1                                    =      0\n",
    "        if debug2:\n",
    "            self._debug2                                    =      debug2\n",
    "        else:\n",
    "            self._debug2                                    =      0\n",
    "        if debug3:\n",
    "            self._debug3                                    =      debug3\n",
    "        else:\n",
    "            self._debug3                                    =      0\n",
    "        self._root_node                                     =      None\n",
    "        self._probability_cache                             =      {}\n",
    "        self._entropy_cache                                 =      {}\n",
    "        self._training_data_dict                            =      {}\n",
    "        self._features_and_values_dict                      =      {}\n",
    "        self._features_and_unique_values_dict               =      {}\n",
    "        self._samples_class_label_dict                      =      {}  \n",
    "        self._class_names                                   =      []\n",
    "        self._class_priors_dict                             =      {}\n",
    "        self._feature_names                                 =      []\n",
    "        self._numeric_features_valuerange_dict              =      {}\n",
    "        self._sampling_points_for_numeric_feature_dict      =      {}\n",
    "        self._feature_values_how_many_uniques_dict          =      {}\n",
    "        self._prob_distribution_numeric_features_dict       =      {}\n",
    "        self._histogram_delta_dict                          =      {}\n",
    "        self._num_of_histogram_bins_dict                    =      {}\n",
    "\n",
    "    def get_training_data(self):\n",
    "        if not self._training_datafile.endswith('.csv'): \n",
    "            TypeError(\"Aborted. get_training_data_from_csv() is only for CSV files\")\n",
    "        class_names = []\n",
    "        all_record_ids_with_class_labels = {}\n",
    "        firstline = None\n",
    "        data_dict = {}\n",
    "        with open(self._training_datafile) as f:\n",
    "            for i,line in enumerate(f):\n",
    "                record = cleanup_csv(line) if self._csv_cleanup_needed else line\n",
    "                if i == 0:\n",
    "                    firstline = record\n",
    "                    continue\n",
    "                parts = record.rstrip().split(r',')\n",
    "                data_dict[parts[0].strip('\"')] = parts[1:]\n",
    "                class_names.append(parts[self._csv_class_column_index])\n",
    "                all_record_ids_with_class_labels[parts[0].strip('\"')] = parts[self._csv_class_column_index]\n",
    "                if i%10000 == 0:\n",
    "                    print('.'),\n",
    "                    sys.stdout.flush()\n",
    "                sys.stdout = sys.__stdout__\n",
    "            f.close() \n",
    "        self._how_many_total_training_samples = i   # i is less by 1 from total num of records; but that's okay\n",
    "        unique_class_names = list(set(class_names))\n",
    "        if self._debug1:\n",
    "            print(\"\\n\\nTotal number of training samples: %d\\n\" % self._how_many_total_training_samples)\n",
    "        all_feature_names = firstline.rstrip().split(',')[1:]\n",
    "        class_column_heading = all_feature_names[self._csv_class_column_index - 1]\n",
    "        feature_names = [all_feature_names[i-1] for i in self._csv_columns_for_features]\n",
    "        class_for_sample_dict = { \"sample_\" + key : \n",
    "               class_column_heading + \"=\" + data_dict[key][self._csv_class_column_index - 1] for key in data_dict}\n",
    "        sample_names = [\"sample_\" + key for key in data_dict]\n",
    "        feature_values_for_samples_dict = {\"sample_\" + key :         \n",
    "                  list(map(operator.add, list(map(operator.add, feature_names, \"=\" * len(feature_names))), \n",
    "           [str(convert(data_dict[key][i-1])) for i in self._csv_columns_for_features])) \n",
    "                           for key in data_dict}\n",
    "        features_and_values_dict = {all_feature_names[i-1] :\n",
    "            [convert(data_dict[key][i-1]) for key in data_dict] for i in self._csv_columns_for_features}\n",
    "        all_class_names = sorted(list(set(class_for_sample_dict.values())))\n",
    "        if self._debug1: print(\"\\n All class names: \"+ str(all_class_names))\n",
    "        numeric_features_valuerange_dict = {}\n",
    "        feature_values_how_many_uniques_dict = {}\n",
    "        features_and_unique_values_dict = {}\n",
    "        for feature in features_and_values_dict:\n",
    "            unique_values_for_feature = list(set(features_and_values_dict[feature]))\n",
    "            unique_values_for_feature = sorted(list(filter(lambda x: x != 'NA', unique_values_for_feature)))\n",
    "            feature_values_how_many_uniques_dict[feature] = len(unique_values_for_feature)\n",
    "            if all(isinstance(x,float) for x in unique_values_for_feature):\n",
    "                numeric_features_valuerange_dict[feature] = \\\n",
    "                                             [min(unique_values_for_feature), max(unique_values_for_feature)]\n",
    "                unique_values_for_feature.sort(key=float)\n",
    "            features_and_unique_values_dict[feature] = sorted(unique_values_for_feature)\n",
    "        if self._debug1:\n",
    "            print(\"\\nAll class names: \" + str(all_class_names))\n",
    "            print(\"\\nEach sample data record:\")\n",
    "            for item in sorted(feature_values_for_samples_dict.items(), key = lambda x: sample_index(x[0]) ):\n",
    "                print(item[0]  + \"  =>  \"  + str(item[1]))\n",
    "            print(\"\\nclass label for each data sample:\")\n",
    "            for item in sorted(class_for_sample_dict.items(), key=lambda x: sample_index(x[0])):\n",
    "                print(item[0]  + \"  =>  \"  + str(item[1]))\n",
    "            print(\"\\nfeatures and the values taken by them:\")\n",
    "            for item in sorted(features_and_values_dict.items()):\n",
    "                print(item[0]  + \"  =>  \"  + str(item[1]))\n",
    "            print(\"\\nnumeric features and their ranges:\")\n",
    "            for item in sorted(numeric_features_valuerange_dict.items()):\n",
    "                print(item[0]  + \"  =>  \"  + str(item[1]))\n",
    "            print(\"\\nnumber of unique values in each feature:\")\n",
    "            for item in sorted(feature_values_how_many_uniques_dict.items()):\n",
    "                print(item[0]  + \"  =>  \"  + str(item[1]))\n",
    "        self._class_names = all_class_names\n",
    "        self._feature_names = feature_names\n",
    "        self._samples_class_label_dict    =  class_for_sample_dict\n",
    "        self._training_data_dict          =  feature_values_for_samples_dict\n",
    "        self._features_and_values_dict    =  features_and_values_dict\n",
    "        self._features_and_unique_values_dict    =  features_and_unique_values_dict\n",
    "        self._numeric_features_valuerange_dict = numeric_features_valuerange_dict\n",
    "        self._feature_values_how_many_uniques_dict = feature_values_how_many_uniques_dict\n",
    "\n",
    "    def calculate_first_order_probabilities(self):\n",
    "        print(\"\\nEstimating probabilities...\")\n",
    "        for feature in self._feature_names:\n",
    "            self.probability_of_feature_value(feature,None)\n",
    "            if self._debug2:\n",
    "                if feature in self._prob_distribution_numeric_features_dict:\n",
    "                    print(\"\\nPresenting probability distribution for a feature considered to be numeric:\")\n",
    "                    for sampling_point in \\\n",
    "                         sorted(self._prob_distribution_numeric_features_dict[feature].keys()):\n",
    "                        print(feature + '::' + str(sampling_point) + ' = ' +  \"{0:.5f}\".format(\n",
    "                               self._prob_distribution_numeric_features_dict[feature][sampling_point]))  \n",
    "                else:\n",
    "                    print(\"\\nPresenting probabilities for the values of a feature considered to be symbolic:\")\n",
    "                    values_for_feature = self._features_and_unique_values_dict[feature]\n",
    "                    for value in values_for_feature:\n",
    "                        prob = self.probability_of_feature_value(feature,value) \n",
    "                        print(feature + '::' + str(value) + ' = ' +  \"{0:.5f}\".format(prob))\n",
    "\n",
    "    def show_training_data(self):\n",
    "        print(\"Class names: %s\" % str(self._class_names))\n",
    "        print(\"\\n\\nFeatures and Their Values:\\n\\n\")\n",
    "        features = self._features_and_values_dict.keys()\n",
    "        for feature in sorted(features):\n",
    "            nums,strings = [x for x in self._features_and_values_dict[feature] if isinstance(x,(float,int))],[x for x in self._features_and_values_dict[feature] if not isinstance(x,(float,int))]\n",
    "            print(\"%s ---> %s\"  % (feature, sorted(nums) + strings))\n",
    "        print(\"\\n\\nSamples vs. Class Labels:\\n\\n\")\n",
    "        for item in sorted(self._samples_class_label_dict.items(), key = lambda x: sample_index(x[0]) ):\n",
    "            print(item)\n",
    "        print(\"\\n\\nTraining Samples:\\n\\n\")\n",
    "        for item in sorted(self._training_data_dict.items(), key = lambda x: sample_index(x[0]) ):\n",
    "            print(item)\n",
    "\n",
    "#-----------------------------    Classify with Decision Tree  --------------------------------\n",
    "\n",
    "    def classify(self, root_node, features_and_values):\n",
    "        '''\n",
    "        Classifies one test sample at a time using the decision tree constructed from\n",
    "        your training file.  The data record for the test sample must be supplied as\n",
    "        shown in the scripts in the `Examples' subdirectory.  See the scripts\n",
    "        construct_dt_and_classify_one_sample_caseX.py in that subdirectory.\n",
    "        '''\n",
    "        if not self._check_names_used(features_and_values):\n",
    "            raise SyntaxError(\"\\n\\nError in the names you have used for features and/or values.  Try using the csv_cleanup_needed option in the constructor call.\") \n",
    "        new_features_and_values = []\n",
    "        pattern = r'(\\S+)\\s*=\\s*(\\S+)'\n",
    "        for feature_and_value in features_and_values:\n",
    "            m = re.search(pattern, feature_and_value)\n",
    "            feature,value = m.group(1),m.group(2)\n",
    "            value = convert(value)\n",
    "            newvalue = value\n",
    "            if ((feature not in self._prob_distribution_numeric_features_dict) and\n",
    "                (all(isinstance(x,float) for x in self._features_and_unique_values_dict[feature]))):\n",
    "                newvalue = closest_sampling_point(value, self._features_and_unique_values_dict[feature])\n",
    "            new_features_and_values.append(feature + \" = \" + str(newvalue))\n",
    "        features_and_values = new_features_and_values       \n",
    "        if self._debug3: print(\"\\nCL1 New feature and values: \"+ str(features_and_values))\n",
    "        answer = {class_name : None for class_name in self._class_names}\n",
    "        answer['solution_path'] = []\n",
    "        classification = self.recursive_descent_for_classification(root_node,features_and_values, answer)\n",
    "        answer['solution_path'].reverse()\n",
    "        if self._debug3: \n",
    "            print(\"\\nCL2 The classification:\")\n",
    "            for class_name in self._class_names:\n",
    "                print(\"    \" + class_name + \" with probability \" + str(classification[class_name]))\n",
    "        classification_for_display = {}\n",
    "        for item in classification:\n",
    "            if isinstance(classification[item], float):\n",
    "                classification_for_display[item] = \"%0.3f\" % classification[item]\n",
    "            else:\n",
    "                classification_for_display[item] =  [\"NODE\" + str(x) for x in classification[item]]\n",
    "        return classification_for_display\n",
    "\n",
    "    def recursive_descent_for_classification(self, node, feature_and_values, answer):\n",
    "        children = node.get_children()\n",
    "        if len(children) == 0:\n",
    "            leaf_node_class_probabilities = node.get_class_probabilities()\n",
    "            for i in range(len(self._class_names)):\n",
    "                answer[self._class_names[i]] = leaf_node_class_probabilities[i]\n",
    "            answer['solution_path'].append(node.get_serial_num())\n",
    "            return answer\n",
    "        feature_tested_at_node = node.get_feature()\n",
    "        if self._debug3: print(\"\\nCLRD1 Feature tested at node for classification: \" + feature_tested_at_node)\n",
    "        value_for_feature = None\n",
    "        path_found = None\n",
    "        pattern = r'(\\S+)\\s*=\\s*(\\S+)'\n",
    "        feature,value = None,None\n",
    "        for feature_and_value in feature_and_values:\n",
    "            m = re.search(pattern, feature_and_value)\n",
    "            feature,value = m.group(1),m.group(2)\n",
    "            if feature == feature_tested_at_node:\n",
    "                value_for_feature = convert(value)\n",
    "        # added in 3.2.0:\n",
    "        if value_for_feature is None:\n",
    "            leaf_node_class_probabilities = node.get_class_probabilities()\n",
    "            for i in range(len(self._class_names)):\n",
    "                answer[self._class_names[i]] = leaf_node_class_probabilities[i]\n",
    "            answer['solution_path'].append(node.get_serial_num())\n",
    "            return answer\n",
    "        if feature_tested_at_node in self._prob_distribution_numeric_features_dict:\n",
    "            if self._debug3: print( \"\\nCLRD2 In the truly numeric section\")\n",
    "            for child in children:\n",
    "                branch_features_and_values = child.get_branch_features_and_values_or_thresholds()\n",
    "                last_feature_and_value_on_branch = branch_features_and_values[-1] \n",
    "                pattern1 = r'(.+)<(.+)'\n",
    "                pattern2 = r'(.+)>(.+)'\n",
    "                if re.search(pattern1, last_feature_and_value_on_branch):\n",
    "                    m = re.search(pattern1, last_feature_and_value_on_branch)\n",
    "                    feature,threshold = m.group(1),m.group(2)\n",
    "                    if value_for_feature <= float(threshold):\n",
    "                        path_found = True\n",
    "                        answer = self.recursive_descent_for_classification(child, feature_and_values, answer)\n",
    "                        answer['solution_path'].append(node.get_serial_num())\n",
    "                        break\n",
    "                if re.search(pattern2, last_feature_and_value_on_branch):\n",
    "                    m = re.search(pattern2, last_feature_and_value_on_branch)\n",
    "                    feature,threshold = m.group(1),m.group(2)\n",
    "                    if value_for_feature > float(threshold):\n",
    "                        path_found = True\n",
    "                        answer = self.recursive_descent_for_classification(child, feature_and_values, answer)\n",
    "                        answer['solution_path'].append(node.get_serial_num())\n",
    "                        break\n",
    "            if path_found: return answer \n",
    "        else:\n",
    "            feature_value_combo = \"\".join([feature_tested_at_node,\"=\", str(convert(value_for_feature))])\n",
    "            if self._debug3: \n",
    "                print(\"\\nCLRD3 In the symbolic section with feature_value_combo: \" + feature_value_combo)\n",
    "            for child in children:\n",
    "                branch_features_and_values = child.get_branch_features_and_values_or_thresholds()\n",
    "                if self._debug3: print(\"\\nCLRD4 branch features and values: \"+str(branch_features_and_values))\n",
    "                last_feature_and_value_on_branch = branch_features_and_values[-1] \n",
    "                if last_feature_and_value_on_branch == feature_value_combo:\n",
    "                    answer = self.recursive_descent_for_classification(child, feature_and_values, answer)\n",
    "                    answer['solution_path'].append(node.get_serial_num())\n",
    "                    path_found = True\n",
    "                    break\n",
    "            if path_found: return answer\n",
    "        if not path_found:\n",
    "            leaf_node_class_probabilities = node.get_class_probabilities()\n",
    "            for i in range(0, len(self._class_names)):\n",
    "                answer[self._class_names[i]] = leaf_node_class_probabilities[i]\n",
    "            answer['solution_path'].append(node.get_serial_num())\n",
    "        return answer\n",
    "\n",
    "    def classify_by_asking_questions(self, root_node):\n",
    "        '''\n",
    "        If you want classification to be carried out by engaging a human user in a\n",
    "        question-answer session, this is the method to use for that purpose.  See the\n",
    "        script classify_by_asking_questions.py in the Examples subdirectory for an\n",
    "        illustration of how to do that.\n",
    "        '''\n",
    "        answer = {class_name : None for class_name in self._class_names}\n",
    "        answer['solution_path'] = []\n",
    "        scratchpad_for_numeric_answers = {feature : None \n",
    "                                   for feature in self._prob_distribution_numeric_features_dict}\n",
    "        classification = self.interactive_recursive_descent_for_classification(root_node, \n",
    "                                                                   answer, scratchpad_for_numeric_answers)\n",
    "        classification['solution_path'].reverse()\n",
    "        classification_for_display = {}\n",
    "        for item in classification:\n",
    "            if isinstance(classification[item], float):\n",
    "                classification_for_display[item] = \"%0.3f\" % classification[item]\n",
    "            else:\n",
    "                classification_for_display[item] =  [\"NODE\" + str(x) for x in classification[item]]\n",
    "        return classification_for_display\n",
    "\n",
    "    def interactive_recursive_descent_for_classification(self, node, answer, scratchpad_for_numerics):\n",
    "        pattern1 = r'(.+)<(.+)'\n",
    "        pattern2 = r'(.+)>(.+)'\n",
    "        user_value_for_feature = None\n",
    "        children = node.get_children()\n",
    "        if len(children) == 0:\n",
    "            leaf_node_class_probabilities = node.get_class_probabilities()\n",
    "            for i in range(len(self._class_names)):\n",
    "                answer[self._class_names[i]] = leaf_node_class_probabilities[i]\n",
    "            answer['solution_path'].append(node.get_serial_num())\n",
    "            return answer\n",
    "        list_of_branch_attributes_to_children = []\n",
    "        for child in children:\n",
    "            branch_features_and_values = child.get_branch_features_and_values_or_thresholds()\n",
    "            feature_and_value_on_branch = branch_features_and_values[-1] \n",
    "            list_of_branch_attributes_to_children.append(feature_and_value_on_branch)\n",
    "        feature_tested_at_node = node.get_feature()\n",
    "        feature_value_combo = None\n",
    "        path_found = None\n",
    "        if feature_tested_at_node in self._prob_distribution_numeric_features_dict:\n",
    "            if scratchpad_for_numerics[feature_tested_at_node]:\n",
    "                user_value_for_feature = scratchpad_for_numerics[feature_tested_at_node]\n",
    "            else:\n",
    "                valuerange =  self._numeric_features_valuerange_dict[feature_tested_at_node]\n",
    "                while True: \n",
    "                    if sys.version_info[0] == 3:\n",
    "                        user_value_for_feature = input( \"\\nWhat is the value for the feature '\" + \n",
    "                       feature_tested_at_node + \"'?\" + \"\\n\" +  \n",
    "                       \"Enter a value in the range: \" + str(valuerange) + \" => \" )\n",
    "                    else:\n",
    "                        user_value_for_feature = raw_input( \"\\nWhat is the value for the feature '\" + \n",
    "                       feature_tested_at_node + \"'?\" + \"\\n\" +    \n",
    "                       \"Enter a value in the range: \" + str(valuerange) + \" => \" )\n",
    "                    user_value_for_feature = convert(user_value_for_feature.strip())\n",
    "                    answer_found = 0\n",
    "                    if valuerange[0] <= user_value_for_feature <= valuerange[1]:\n",
    "                        answer_found = 1\n",
    "                        break    \n",
    "                    if answer_found == 1: break\n",
    "                    print(\"You entered illegal value. Let's try again\")\n",
    "                scratchpad_for_numerics[feature_tested_at_node] = user_value_for_feature\n",
    "            for i in range(len(list_of_branch_attributes_to_children)):\n",
    "                branch_attribute = list_of_branch_attributes_to_children[i]\n",
    "                if re.search(pattern1, branch_attribute):\n",
    "                    m = re.search(pattern1, branch_attribute)\n",
    "                    feature,threshold = m.group(1),m.group(2)\n",
    "                    if user_value_for_feature <= float(threshold):\n",
    "                        answer = self.interactive_recursive_descent_for_classification(children[i], \n",
    "                                                                          answer, scratchpad_for_numerics)\n",
    "                        path_found = True\n",
    "                        answer['solution_path'].append(node.get_serial_num())\n",
    "                        break\n",
    "                if re.search(pattern2, branch_attribute):\n",
    "                    m = re.search(pattern2, branch_attribute)\n",
    "                    feature,threshold = m.group(1),m.group(2)\n",
    "                    if user_value_for_feature > float(threshold):\n",
    "                        answer = self.interactive_recursive_descent_for_classification(children[i], \n",
    "                                                                          answer, scratchpad_for_numerics)\n",
    "                        answer['solution_path'].append(node.get_serial_num())\n",
    "                        break\n",
    "            if path_found: return answer\n",
    "        else:\n",
    "            possible_values_for_feature = self._features_and_unique_values_dict[feature_tested_at_node]\n",
    "            while True:\n",
    "                if sys.version_info[0] == 3:\n",
    "                    user_value_for_feature = \\\n",
    "                       input( \"\\nWhat is the value for the feature '\" + feature_tested_at_node + \n",
    "                              \"'?\" + \"\\n\" + \"Enter one of: \" + str(possible_values_for_feature) + \" => \" )\n",
    "                else:\n",
    "                    user_value_for_feature = \\\n",
    "                       raw_input( \"\\nWhat is the value for the feature '\" + feature_tested_at_node + \n",
    "                                  \"'?\" + \"\\n\" + \"Enter one of: \" + str(possible_values_for_feature) + \" => \" )\n",
    "                user_value_for_feature = convert(user_value_for_feature.strip())\n",
    "                answer_found = 0\n",
    "                for value in possible_values_for_feature:\n",
    "                    if value == user_value_for_feature: \n",
    "                        answer_found = 1\n",
    "                        break\n",
    "                if answer_found == 1: break\n",
    "                print(\"You entered illegal value. Let's try again\")\n",
    "            feature_value_combo = \"\".join([feature_tested_at_node,\"=\",str(user_value_for_feature)])\n",
    "            for i in range(len(list_of_branch_attributes_to_children)):\n",
    "                branch_attribute = list_of_branch_attributes_to_children[i]\n",
    "                if branch_attribute == feature_value_combo:\n",
    "                    answer = self.interactive_recursive_descent_for_classification(children[i], \n",
    "                                                                          answer, scratchpad_for_numerics)\n",
    "                    path_found = True\n",
    "                    answer['solution_path'].append(node.get_serial_num())\n",
    "                    break\n",
    "            if path_found: return answer    \n",
    "        if not path_found:\n",
    "            leaf_node_class_probabilities = node.get_class_probabilities()\n",
    "            for i in range(0, len(self._class_names)):\n",
    "                answer[self._class_names[i]] = leaf_node_class_probabilities[i]\n",
    "            answer['solution_path'].append(node.get_serial_num())\n",
    "\n",
    "        return answer        \n",
    "\n",
    "##-------------------------------  Construct Decision Tree  ------------------------------------\n",
    "\n",
    "    def construct_decision_tree_classifier(self):\n",
    "        '''\n",
    "        Construct the root node object and set its entropy value as derived from the priors\n",
    "        associated with the different classes.\n",
    "        '''        \n",
    "        print(\"\\nConstructing a decision tree...\")\n",
    "        if self._debug3:        \n",
    "            self.determine_data_condition() \n",
    "            print(\"\\nStarting construction of the decision tree:\\n\") \n",
    "        class_probabilities = list(map(lambda x: self.prior_probability_for_class(x), self._class_names))\n",
    "        if self._debug3:         \n",
    "            print(\"\\nPrior class probabilities: \" + str(class_probabilities))\n",
    "            print(\"\\nClass names: \" + str(self._class_names))\n",
    "        entropy = self.class_entropy_on_priors()\n",
    "        if self._debug3: print(\"\\nClass entropy on priors: \"+ str(entropy))\n",
    "#        root_node = self.DTNode(None, entropy, class_probabilities, [], self, 'root')\n",
    "        root_node = DTNode(None, entropy, class_probabilities, [], self, 'root')\n",
    "        root_node.set_class_names(self._class_names)\n",
    "        self._root_node = root_node\n",
    "        self.recursive_descent(root_node)\n",
    "        return root_node        \n",
    "\n",
    "    def recursive_descent(self, node):\n",
    "        '''\n",
    "        After the root node of the decision tree is constructed by the previous method, we\n",
    "        find  at that node the feature that yields the greatest reduction in class entropy\n",
    "        from the entropy based on just the class priors. The logic for finding this\n",
    "        feature is different for symbolic features and for numeric features (that logic is\n",
    "        built into the best feature calculator). We then invoke this method recursively to \n",
    "        create the rest of the tree.  \n",
    "        '''\n",
    "        if self._debug3:\n",
    "            print(\"\\n==================== ENTERING RECURSIVE DESCENT ==========================\")\n",
    "        node_serial_number = node.get_serial_num()\n",
    "        features_and_values_or_thresholds_on_branch = node.get_branch_features_and_values_or_thresholds()\n",
    "        existing_node_entropy = node.get_node_entropy()\n",
    "        if self._debug3: \n",
    "            print(\"\\nRD1 NODE SERIAL NUMBER: \"+ str(node_serial_number))\n",
    "            print(\"\\nRD2 Existing Node Entropy: \" + str(existing_node_entropy))\n",
    "            print(\"\\nRD3 features_and_values_or_thresholds_on_branch: \" + \n",
    "                                                        str(features_and_values_or_thresholds_on_branch))\n",
    "            class_probs = node.get_class_probabilities()\n",
    "            print(\"\\nRD4 Class probabilities: \" + str(class_probs))\n",
    "        if existing_node_entropy < self._entropy_threshold: \n",
    "            if self._debug3: print(\"\\nRD5 returning because existing node entropy is below threshold\")\n",
    "            return\n",
    "        copy_of_path_attributes = deep_copy_array(features_and_values_or_thresholds_on_branch)\n",
    "        best_feature,best_feature_entropy,best_feature_val_entropies,decision_val = \\\n",
    "                       self.best_feature_calculator(copy_of_path_attributes, existing_node_entropy)\n",
    "        node.set_feature(best_feature)\n",
    "        if self._debug3: node.display_node() \n",
    "        if self._max_depth_desired is not None and \\\n",
    "         len(features_and_values_or_thresholds_on_branch) >= self._max_depth_desired:\n",
    "            if self._debug3: print(\"\\nRD6 REACHED LEAF NODE AT MAXIMUM DEPTH ALLOWED\")\n",
    "            return\n",
    "        if best_feature is None: return\n",
    "        if self._debug3:\n",
    "            print(\"\\nRD7 Existing entropy at node: \" + str(existing_node_entropy))\n",
    "            print(\"\\nRD8 Calculated best feature is %s and its value %s\" % (best_feature, decision_val))\n",
    "            print(\"\\nRD9 Best feature entropy: \"+ str(best_feature_entropy))\n",
    "            print(\"\\nRD10 Calculated entropies for different values of best feature: %s\" % str(best_feature_val_entropies))\n",
    "        entropy_gain = existing_node_entropy - best_feature_entropy\n",
    "        if self._debug3: print(\"\\nRD11 Expected entropy gain at this node: \" + str(entropy_gain))\n",
    "        if entropy_gain > self._entropy_threshold:\n",
    "            if best_feature in self._numeric_features_valuerange_dict and \\\n",
    "                         self._feature_values_how_many_uniques_dict[best_feature] > \\\n",
    "                                             self._symbolic_to_numeric_cardinality_threshold:\n",
    "                best_threshold = decision_val                 # as returned by best feature calculator\n",
    "                best_entropy_for_less, best_entropy_for_greater = best_feature_val_entropies\n",
    "                extended_branch_features_and_values_or_thresholds_for_lessthan_child = \\\n",
    "                                  deep_copy_array(features_and_values_or_thresholds_on_branch)\n",
    "                extended_branch_features_and_values_or_thresholds_for_greaterthan_child  = \\\n",
    "                                  deep_copy_array(features_and_values_or_thresholds_on_branch)\n",
    "                feature_threshold_combo_for_less_than = \\\n",
    "                                            \"\".join([best_feature,\"<\",str(convert(best_threshold))])\n",
    "                feature_threshold_combo_for_greater_than = \\\n",
    "                                            \"\".join([best_feature,\">\",str(convert(best_threshold))])\n",
    "                extended_branch_features_and_values_or_thresholds_for_lessthan_child.append( \n",
    "                                                              feature_threshold_combo_for_less_than)\n",
    "                extended_branch_features_and_values_or_thresholds_for_greaterthan_child.append( \n",
    "                                                           feature_threshold_combo_for_greater_than)\n",
    "                if self._debug3:\n",
    "                    print(\"\\nRD12 extended_branch_features_and_values_or_thresholds_for_lessthan_child: \"+ \n",
    "                             str(extended_branch_features_and_values_or_thresholds_for_lessthan_child))\n",
    "                    print(\"\\nRD13 extended_branch_features_and_values_or_thresholds_for_greaterthan_child: \" +\n",
    "                             str(extended_branch_features_and_values_or_thresholds_for_greaterthan_child))\n",
    "                class_probabilities_for_lessthan_child_node = list(map(lambda x: \n",
    "                       self.probability_of_a_class_given_sequence_of_features_and_values_or_thresholds(\\\n",
    "                      x, extended_branch_features_and_values_or_thresholds_for_lessthan_child), self._class_names))\n",
    "                class_probabilities_for_greaterthan_child_node = list(map(lambda x: \n",
    "                   self.probability_of_a_class_given_sequence_of_features_and_values_or_thresholds(\n",
    "                      x, extended_branch_features_and_values_or_thresholds_for_greaterthan_child), \n",
    "                                                           self._class_names))\n",
    "                if self._debug3:\n",
    "                    print(\"\\nRD14 class entropy for going down lessthan child: \" + str(best_entropy_for_less))\n",
    "                    print(\"\\nRD15 class_entropy_for_going_down_greaterthan_child: \" + \n",
    "                                                                        str(best_entropy_for_greater))\n",
    "                if best_entropy_for_less < existing_node_entropy - self._entropy_threshold:\n",
    "#                    left_child_node = self.DTNode(None, best_entropy_for_less, \n",
    "                    left_child_node = DTNode(None, best_entropy_for_less, \n",
    "                                      class_probabilities_for_lessthan_child_node,\n",
    "                                         extended_branch_features_and_values_or_thresholds_for_lessthan_child, self)\n",
    "                    node.add_child_link(left_child_node)\n",
    "                    self.recursive_descent(left_child_node)\n",
    "                if best_entropy_for_greater < existing_node_entropy - self._entropy_threshold:\n",
    "#                    right_child_node = self.DTNode(None, best_entropy_for_greater,\n",
    "                    right_child_node = DTNode(None, best_entropy_for_greater,\n",
    "                                      class_probabilities_for_greaterthan_child_node, \n",
    "                                      extended_branch_features_and_values_or_thresholds_for_greaterthan_child, self)\n",
    "                    node.add_child_link(right_child_node)\n",
    "                    self.recursive_descent(right_child_node)\n",
    "            else:\n",
    "                if self._debug3:\n",
    "                    print(\"\\nRD16 RECURSIVE DESCENT: In section for symbolic features for creating children\")\n",
    "                values_for_feature = self._features_and_unique_values_dict[best_feature]\n",
    "                if self._debug3:\n",
    "                    print(\"\\nRD17 Values for feature %s are %s\" % (best_feature, str(values_for_feature)))\n",
    "                feature_value_combos = \\\n",
    "                  map(lambda x: \"\".join([best_feature,\"=\",x]), map(str, map(convert, values_for_feature)))\n",
    "                feature_value_combos = sorted(feature_value_combos)\n",
    "                class_entropies_for_children = []\n",
    "                for feature_and_value_index in range(len(feature_value_combos)):\n",
    "                    if self._debug3:\n",
    "                        print(\"\\nRD18 Creating a child node for: \" + \n",
    "                                                       str(feature_value_combos[feature_and_value_index])) \n",
    "                    extended_branch_features_and_values_or_thresholds = None\n",
    "                    if features_and_values_or_thresholds_on_branch is None:\n",
    "                        extended_branch_features_and_values_or_thresholds = \\\n",
    "                                                 [feature_value_combos[feature_and_value_index]]\n",
    "                    else:\n",
    "                        extended_branch_features_and_values_or_thresholds = \\\n",
    "                            deep_copy_array(features_and_values_or_thresholds_on_branch)\n",
    "                        extended_branch_features_and_values_or_thresholds.append(\n",
    "                                                 feature_value_combos[feature_and_value_index])\n",
    "                    class_probabilities = list(map(lambda x: \n",
    "                       self.probability_of_a_class_given_sequence_of_features_and_values_or_thresholds(\n",
    "                                 x, extended_branch_features_and_values_or_thresholds), self._class_names))\n",
    "                    class_entropy_for_child = \\\n",
    "                          self.class_entropy_for_a_given_sequence_of_features_and_values_or_thresholds(\n",
    "                                                        extended_branch_features_and_values_or_thresholds)\n",
    "                    if self._debug3: \n",
    "                        print(\"\\nRD19 branch attributes: \"+ \n",
    "                                                 str(extended_branch_features_and_values_or_thresholds))\n",
    "                        print(\"\\nRD20 class entropy for child: \" + str(class_entropy_for_child)) \n",
    "                    if existing_node_entropy - class_entropy_for_child > self._entropy_threshold:\n",
    "#                        child_node = self.DTNode(None, class_entropy_for_child, \n",
    "                        child_node = DTNode(None, class_entropy_for_child, \n",
    "                             class_probabilities, extended_branch_features_and_values_or_thresholds, self)\n",
    "                        node.add_child_link( child_node )\n",
    "                        self.recursive_descent(child_node)\n",
    "                    elif self._debug3: print(\"\\nRD21 This child will NOT result in a node\")\n",
    "        else:\n",
    "            if self._debug3:\n",
    "                print(\"\\nRD22 REACHED LEAF NODE NATURALLY for: \" + \n",
    "                                      str(features_and_values_or_thresholds_on_branch))\n",
    "            return   \n",
    "\n",
    "    def best_feature_calculator(self, features_and_values_or_thresholds_on_branch, existing_node_entropy):\n",
    "        '''\n",
    "        This is the heart of the decision tree constructor.  Its main job is to figure\n",
    "        out the best feature to use for partitioning the training data samples that\n",
    "        correspond to the current node.  The search for the best feature is carried\n",
    "        out differently for symbolic features and for numeric features.  For a\n",
    "        symbolic feature, the method estimates the entropy for each value of the\n",
    "        feature and then averages out these entropies as a measure of the\n",
    "        discriminatory power of that features.  For a numeric feature, on the other\n",
    "        hand, it estimates the entropy reduction that can be achieved if we were to\n",
    "        partition the set of training samples at each possible threshold for that\n",
    "        numeric feature.  For a numeric feature, all possible sampling points\n",
    "        relevant to the node in question are considered as candidates for thresholds.\n",
    "        '''\n",
    "        pattern1 = r'(.+)=(.+)'\n",
    "        pattern2 = r'(.+)<(.+)'\n",
    "        pattern3 = r'(.+)>(.+)'\n",
    "        all_symbolic_features = []\n",
    "        for feature_name in self._feature_names:\n",
    "            if feature_name not in self._prob_distribution_numeric_features_dict:\n",
    "                all_symbolic_features.append(feature_name)\n",
    "        symbolic_features_already_used = []\n",
    "        for feature_and_value_or_threshold in features_and_values_or_thresholds_on_branch:\n",
    "            if re.search(pattern1, feature_and_value_or_threshold):\n",
    "                m = re.search(pattern1, feature_and_value_or_threshold)\n",
    "                feature = m.group(1)\n",
    "                symbolic_features_already_used.append(feature)\n",
    "        symbolic_features_not_yet_used = [x for x in all_symbolic_features if x not in symbolic_features_already_used]\n",
    "        true_numeric_types = []        \n",
    "        symbolic_types = []\n",
    "        true_numeric_types_feature_names = []\n",
    "        symbolic_types_feature_names = []\n",
    "        for item in features_and_values_or_thresholds_on_branch:\n",
    "            if re.search(pattern2, item):\n",
    "                true_numeric_types.append(item)\n",
    "                m = re.search(pattern2, item)\n",
    "                feature,value = m.group(1),m.group(2)\n",
    "                true_numeric_types_feature_names.append(feature)\n",
    "            elif re.search(pattern3, item): \n",
    "                true_numeric_types.append(item)\n",
    "                m = re.search(pattern3, item)\n",
    "                feature,value = m.group(1),m.group(2)\n",
    "                true_numeric_types_feature_names.append(feature)\n",
    "            else:\n",
    "                symbolic_types.append(item) \n",
    "                m = re.search(pattern1, item)\n",
    "                feature,value = m.group(1),m.group(2)\n",
    "                symbolic_types_feature_names.append(feature)\n",
    "        true_numeric_types_feature_names = list(set(true_numeric_types_feature_names))\n",
    "        symbolic_types_feature_names = list(set(symbolic_types_feature_names))\n",
    "        bounded_intervals_numeric_types = self.find_bounded_intervals_for_numeric_features(true_numeric_types)\n",
    "        # Calculate the upper and the lower bounds to be used when searching for the best\n",
    "        # threshold for each of the numeric features that are in play at the current node:\n",
    "        upperbound = {feature : None for feature in true_numeric_types_feature_names}\n",
    "        lowerbound = {feature : None for feature in true_numeric_types_feature_names}\n",
    "        for item in bounded_intervals_numeric_types:\n",
    "            if item[1] == '>':\n",
    "                lowerbound[item[0]] = float(item[2])\n",
    "            else:\n",
    "                upperbound[item[0]] = float(item[2])\n",
    "        entropy_values_for_different_features = {}\n",
    "        partitioning_point_child_entropies_dict = {feature : {} for feature in self._feature_names}\n",
    "        partitioning_point_threshold = {feature : None for feature in self._feature_names}\n",
    "        entropies_for_different_values_of_symbolic_feature ={feature : [] for feature in self._feature_names}\n",
    "        for i in range(len(self._feature_names)):\n",
    "            feature_name = self._feature_names[i]\n",
    "            if self._debug3: \n",
    "                print(\"\\n\\nBFC1          FEATURE BEING CONSIDERED: \" + feature_name)\n",
    "            if feature_name in symbolic_features_already_used:\n",
    "                continue\n",
    "            elif feature_name in self._numeric_features_valuerange_dict and \\\n",
    "                              self._feature_values_how_many_uniques_dict[feature_name] > \\\n",
    "                                                  self._symbolic_to_numeric_cardinality_threshold:\n",
    "                values = self._sampling_points_for_numeric_feature_dict[feature_name]\n",
    "                if self._debug3: print(\"\\nBFC2 values for %s are %s                \" % (feature_name, values))\n",
    "                newvalues = []\n",
    "                if feature_name in true_numeric_types_feature_names:\n",
    "                    if upperbound[feature_name] is not None and lowerbound[feature_name] is not None and \\\n",
    "                                                 lowerbound[feature_name] >= upperbound[feature_name]:\n",
    "                        continue\n",
    "                    elif upperbound[feature_name] is not None and lowerbound[feature_name] is not None and \\\n",
    "                                                       lowerbound[feature_name] < upperbound[feature_name]:\n",
    "                        newvalues = [x for x in values \\\n",
    "                                             if lowerbound[feature_name] < x <= upperbound[feature_name]]\n",
    "                    elif upperbound[feature_name] is not None:\n",
    "                        newvalues = [x for x in values if x <= upperbound[feature_name]]\n",
    "                    elif lowerbound[feature_name] is not None:\n",
    "                        newvalues = [x for x in values if x > lowerbound[feature_name]]\n",
    "                    else:\n",
    "                        raise Exception(\"Error in bound specifications in best feature calculator\")\n",
    "                else:\n",
    "                    newvalues = values\n",
    "                if len(newvalues) == 0:\n",
    "                    continue\n",
    "                partitioning_entropies = []\n",
    "                for value in newvalues:\n",
    "                    feature_and_less_than_value_string = \"\".join([feature_name,\"<\",str(convert(value))]) \n",
    "                    feature_and_greater_than_value_string = \"\".join([feature_name,\">\",str(convert(value))])\n",
    "                    for_left_child = for_right_child = None\n",
    "                    if features_and_values_or_thresholds_on_branch:\n",
    "                        for_left_child = deep_copy_array(features_and_values_or_thresholds_on_branch)\n",
    "                        for_left_child.append(feature_and_less_than_value_string)\n",
    "                        for_right_child = deep_copy_array(features_and_values_or_thresholds_on_branch)\n",
    "                        for_right_child.append(feature_and_greater_than_value_string)\n",
    "                    else:\n",
    "                        for_left_child = [feature_and_less_than_value_string]\n",
    "                        for_right_child = [feature_and_greater_than_value_string]\n",
    "                    entropy1 = self.class_entropy_for_less_than_threshold_for_feature(\\\n",
    "                                        features_and_values_or_thresholds_on_branch,feature_name,value)\n",
    "                    entropy2 = self.class_entropy_for_greater_than_threshold_for_feature(\\\n",
    "                                        features_and_values_or_thresholds_on_branch,feature_name, value)\n",
    "                    partitioning_entropy = entropy1 * \\\n",
    "                       self.probability_of_a_sequence_of_features_and_values_or_thresholds(for_left_child) \\\n",
    "                           + entropy2 * \\\n",
    "                       self.probability_of_a_sequence_of_features_and_values_or_thresholds(for_right_child)\n",
    "                    partitioning_entropies.append(partitioning_entropy)\n",
    "                    partitioning_point_child_entropies_dict[feature_name][value] = (entropy1, entropy2)\n",
    "                min_entropy,best_partition_point_index = minimum(partitioning_entropies)         \n",
    "                if min_entropy < existing_node_entropy:\n",
    "                    partitioning_point_threshold[feature_name] = newvalues[best_partition_point_index]\n",
    "                    entropy_values_for_different_features[feature_name] = min_entropy\n",
    "            else:          \n",
    "                if self._debug3:\n",
    "                    print(\"\\nBFC3 Best feature calculator: Entering section reserved for symbolic features\")\n",
    "                    print(\"\\nBFC4 Feature name: \" + feature_name)\n",
    "                values =  self._features_and_unique_values_dict[feature_name]\n",
    "                values = sorted(list(set(filter(lambda x: x != 'NA', values))))\n",
    "                if self._debug3: print(\"\\nBFC5 values for feature %s are %s\" % (feature_name, values))\n",
    "                entropy = 0\n",
    "                for value in values:                              \n",
    "                    feature_value_string = feature_name + \"=\" + str(convert(value))\n",
    "                    if self._debug3: print(\"\\nBFC6 feature_value_string: \" + feature_value_string)\n",
    "                    extended_attributes = deep_copy_array(features_and_values_or_thresholds_on_branch)\n",
    "                    if features_and_values_or_thresholds_on_branch:\n",
    "                        extended_attributes.append(feature_value_string)\n",
    "                    else:\n",
    "                        extended_attributes = [feature_value_string]\n",
    "                    entropy += \\\n",
    "         self.class_entropy_for_a_given_sequence_of_features_and_values_or_thresholds(extended_attributes) * \\\n",
    "         self.probability_of_a_sequence_of_features_and_values_or_thresholds(extended_attributes)\n",
    "                    if self._debug3:\n",
    "                        print(\"\\nBFC7 Entropy calculated for symbolic feature value choice (%s, %s) is %s\" % \\\n",
    "                                                                   (feature_name,value,entropy))       \n",
    "                    entropies_for_different_values_of_symbolic_feature[feature_name].append(entropy)\n",
    "                if entropy < existing_node_entropy:\n",
    "                    entropy_values_for_different_features[feature_name] = entropy\n",
    "        min_entropy_for_best_feature = None\n",
    "        best_feature_name = None\n",
    "        # sorting introduced in 3.2.0:\n",
    "        for feature_nom in sorted(entropy_values_for_different_features):\n",
    "            if not best_feature_name:\n",
    "                best_feature_name = feature_nom\n",
    "                min_entropy_for_best_feature = entropy_values_for_different_features[feature_nom]\n",
    "            else:\n",
    "                if entropy_values_for_different_features[feature_nom] < min_entropy_for_best_feature:\n",
    "                    best_feature_name = feature_nom                    \n",
    "                    min_entropy_for_best_feature = entropy_values_for_different_features[feature_nom]\n",
    "        if  best_feature_name in partitioning_point_threshold:\n",
    "            threshold_for_best_feature = partitioning_point_threshold[best_feature_name]\n",
    "        else:\n",
    "            threshold_for_best_feature = None\n",
    "        best_feature_entropy = min_entropy_for_best_feature\n",
    "        val_based_entropies_to_be_returned = None\n",
    "        decision_val_to_be_returned = None\n",
    "        if best_feature_name in self._numeric_features_valuerange_dict and \\\n",
    "                              self._feature_values_how_many_uniques_dict[best_feature_name] > \\\n",
    "                                                    self._symbolic_to_numeric_cardinality_threshold:\n",
    "            val_based_entropies_to_be_returned = \\\n",
    "                 partitioning_point_child_entropies_dict[best_feature_name][threshold_for_best_feature]\n",
    "        else:\n",
    "            val_based_entropies_to_be_returned = None\n",
    "        if  best_feature_name in partitioning_point_threshold:\n",
    "            decision_val_to_be_returned = partitioning_point_threshold[best_feature_name]\n",
    "        else:\n",
    "            decision_val_to_be_returned = None\n",
    "        if self._debug3:\n",
    "            print(\"\\nBFC8 Val based entropies to be returned for feature %s are %s\" % \n",
    "                             (best_feature_name, str(val_based_entropies_to_be_returned)))\n",
    "        return best_feature_name, best_feature_entropy, val_based_entropies_to_be_returned, decision_val_to_be_returned\n",
    "\n",
    "#-----------------------------------  Entropy Calculators  ------------------------------------\n",
    "\n",
    "    def class_entropy_on_priors(self):\n",
    "        if 'priors' in self._entropy_cache:\n",
    "            return self._entropy_cache['priors']\n",
    "        entropy = None\n",
    "        for class_name in self._class_names:\n",
    "            prob = self.prior_probability_for_class(class_name)\n",
    "            if (prob >= 0.0001) and (prob <= 0.999):\n",
    "                log_prob = math.log(prob,2)\n",
    "            if prob < 0.0001:\n",
    "                log_prob = 0 \n",
    "            if prob > 0.999:\n",
    "                log_prob = 0 \n",
    "            if entropy is None:\n",
    "                entropy = -1.0 * prob * log_prob\n",
    "                continue\n",
    "            entropy += -1.0 * prob * log_prob\n",
    "        if abs(entropy) < 0.0000001: entropy = 0.0\n",
    "        self._entropy_cache['priors'] = entropy\n",
    "        return entropy\n",
    "\n",
    "    def entropy_scanner_for_a_numeric_feature(self, feature):\n",
    "        all_sampling_points = self._sampling_points_for_numeric_feature_dict[feature]\n",
    "        entropies_for_less_than_thresholds = []\n",
    "        entropies_for_greater_than_thresholds = []\n",
    "        for point in  all_sampling_points:\n",
    "            entropies_for_less_than_thresholds.append(\n",
    "                    self.class_entropy_for_less_than_threshold_for_feature([], feature, point))\n",
    "            entropies_for_greater_than_thresholds.append(\n",
    "                    self.class_entropy_for_greater_than_threshold_for_feature([], feature, point))\n",
    "        print(\"\\nSCANNER: All entropies less than thresholds for feature %s are: %s\" % \n",
    "                                (feature, entropies_for_less_than_thresholds))\n",
    "        print(\"\\nSCANNER: All entropies greater than thresholds for feature %s are: %s\" % \n",
    "                                (feature, entropies_for_greater_than_thresholds))\n",
    "\n",
    "    def class_entropy_for_less_than_threshold_for_feature(self, \\\n",
    "                array_of_features_and_values_or_thresholds, feature, threshold):\n",
    "        threshold = convert(threshold)\n",
    "        feature_threshold_combo = feature + '<' + str(threshold)\n",
    "        sequence = \":\".join(array_of_features_and_values_or_thresholds) + \":\" + feature_threshold_combo\n",
    "        if sequence in self._entropy_cache:\n",
    "            return self._entropy_cache[sequence]\n",
    "        copy_of_array_of_features_and_values_or_thresholds = \\\n",
    "                        deep_copy_array(array_of_features_and_values_or_thresholds)\n",
    "        copy_of_array_of_features_and_values_or_thresholds.append(feature_threshold_combo)\n",
    "        entropy = None\n",
    "        for class_name in self._class_names:\n",
    "            prob = self.probability_of_a_class_given_sequence_of_features_and_values_or_thresholds(\\\n",
    "                                        class_name, copy_of_array_of_features_and_values_or_thresholds)\n",
    "            if (prob >= 0.0001) and (prob <= 0.999):\n",
    "                log_prob = math.log(prob,2)\n",
    "            if prob < 0.0001:\n",
    "                log_prob = 0 \n",
    "            if prob > 0.999:\n",
    "                log_prob = 0 \n",
    "            if entropy is None:\n",
    "                entropy = -1.0 * prob * log_prob\n",
    "                continue\n",
    "            entropy += -1.0 * prob * log_prob\n",
    "        self._entropy_cache[sequence] = entropy\n",
    "        if abs(entropy) < 0.0000001: entropy = 0.0\n",
    "        return entropy\n",
    "\n",
    "    def class_entropy_for_greater_than_threshold_for_feature(self, \n",
    "                    array_of_features_and_values_or_thresholds, feature, threshold):\n",
    "        threshold = convert(threshold)\n",
    "        feature_threshold_combo = feature + '>' + str(threshold)\n",
    "        sequence = \":\".join(array_of_features_and_values_or_thresholds) + \":\" + feature_threshold_combo\n",
    "        if sequence in self._entropy_cache:\n",
    "            return self._entropy_cache[sequence]\n",
    "        copy_of_array_of_features_and_values_or_thresholds = \\\n",
    "                               deep_copy_array(array_of_features_and_values_or_thresholds)\n",
    "        copy_of_array_of_features_and_values_or_thresholds.append(feature_threshold_combo)\n",
    "        entropy = None\n",
    "        for class_name in self._class_names:\n",
    "            prob = self.probability_of_a_class_given_sequence_of_features_and_values_or_thresholds(\\\n",
    "                                        class_name, copy_of_array_of_features_and_values_or_thresholds)\n",
    "            if (prob >= 0.0001) and (prob <= 0.999):\n",
    "                log_prob = math.log(prob,2)\n",
    "            if prob < 0.0001:\n",
    "                log_prob = 0 \n",
    "            if prob > 0.999:\n",
    "                log_prob = 0 \n",
    "            if entropy is None:\n",
    "                entropy = -1.0 * prob * log_prob\n",
    "                continue\n",
    "            entropy += -1.0 * prob * log_prob\n",
    "        if abs(entropy) < 0.0000001: entropy = 0.0\n",
    "        self._entropy_cache[sequence] = entropy\n",
    "        return entropy\n",
    "\n",
    "    def class_entropy_for_a_given_sequence_of_features_and_values_or_thresholds(self, \n",
    "                                             array_of_features_and_values_or_thresholds):\n",
    "        sequence = \":\".join(array_of_features_and_values_or_thresholds)\n",
    "        if sequence in self._entropy_cache:\n",
    "            return self._entropy_cache[sequence]\n",
    "        entropy = None    \n",
    "        for class_name in self._class_names:\n",
    "            prob = self.probability_of_a_class_given_sequence_of_features_and_values_or_thresholds(\\\n",
    "                                                class_name, array_of_features_and_values_or_thresholds)\n",
    "            if (prob >= 0.0001) and (prob <= 0.999):\n",
    "                log_prob = math.log(prob,2)\n",
    "            if prob < 0.0001:\n",
    "                log_prob = 0 \n",
    "            if prob > 0.999:\n",
    "                log_prob = 0 \n",
    "            if entropy is None:\n",
    "                entropy = -1.0 * prob * log_prob\n",
    "                continue\n",
    "            entropy += -1.0 * prob * log_prob\n",
    "        if abs(entropy) < 0.0000001: entropy = 0.0\n",
    "        self._entropy_cache[sequence] = entropy\n",
    "        return entropy\n",
    "\n",
    "\n",
    "#---------------------------------  Probability Calculators  ----------------------------------\n",
    "\n",
    "    def prior_probability_for_class(self, class_name):\n",
    "        class_name_in_cache = \"\".join([\"prior::\", class_name])\n",
    "        if class_name_in_cache in self._probability_cache:\n",
    "            return self._probability_cache[class_name_in_cache]\n",
    "        total_num_of_samples = len( self._samples_class_label_dict )\n",
    "        all_values = self._samples_class_label_dict.values()\n",
    "        for this_class_name in self._class_names:\n",
    "            trues = list(filter( lambda x: x == this_class_name, all_values ))\n",
    "            prior_for_this_class = (1.0 * len(trues)) / total_num_of_samples\n",
    "            self._class_priors_dict[this_class_name] = prior_for_this_class\n",
    "            this_class_name_in_cache = \"\".join([\"prior::\", this_class_name])\n",
    "            self._probability_cache[this_class_name_in_cache] = prior_for_this_class\n",
    "        return self._probability_cache[class_name_in_cache]\n",
    "\n",
    "    def calculate_class_priors(self):\n",
    "        print(\"\\nEstimating class priors...\")\n",
    "        if len(self._class_priors_dict) > 1: return\n",
    "        for class_name in self._class_names:\n",
    "            class_name_in_cache = \"\".join([\"prior::\", class_name])\n",
    "            total_num_of_samples = len( self._samples_class_label_dict )\n",
    "            all_values = self._samples_class_label_dict.values()\n",
    "            trues = list(filter( lambda x: x == class_name, all_values ))\n",
    "            prior_for_this_class = (1.0 * len(trues)) / total_num_of_samples\n",
    "            self._class_priors_dict[class_name] = prior_for_this_class\n",
    "            this_class_name_in_cache = \"\".join([\"prior::\", class_name])\n",
    "            self._probability_cache[this_class_name_in_cache] = prior_for_this_class\n",
    "        if self._debug2: print(str(self._class_priors_dict))\n",
    "\n",
    "    def probability_of_feature_value(self, feature_name, value):\n",
    "        value = convert(value)\n",
    "        if (value is not None) and (feature_name in self._sampling_points_for_numeric_feature_dict):\n",
    "            value = closest_sampling_point(convert(value), self._sampling_points_for_numeric_feature_dict[feature_name])\n",
    "        if value is not None:\n",
    "            feature_and_value = \"\".join([feature_name, \"=\", str(convert(value))])\n",
    "        if (value is not None) and (feature_and_value in self._probability_cache):\n",
    "            return self._probability_cache[feature_and_value]\n",
    "        histogram_delta = num_of_hist_bins = valuerange = diffrange = None\n",
    "        if feature_name in self._numeric_features_valuerange_dict:\n",
    "            if self._feature_values_how_many_uniques_dict[feature_name] > self._symbolic_to_numeric_cardinality_threshold:\n",
    "                if feature_name not in self._sampling_points_for_numeric_feature_dict:\n",
    "                    valuerange = self._numeric_features_valuerange_dict[feature_name] \n",
    "                    diffrange = valuerange[1] - valuerange[0]\n",
    "                    unique_values_for_feature = sorted(list(set(filter(lambda x: x != 'NA', \n",
    "                                   self._features_and_values_dict[feature_name]))))\n",
    "                    diffs = sorted([unique_values_for_feature[i] - unique_values_for_feature[i-1] \n",
    "                                            for i in range(1,len(unique_values_for_feature))])\n",
    "                    median_diff = diffs[int(len(diffs)/2) - 1]\n",
    "                    histogram_delta =  median_diff * 2\n",
    "                    if histogram_delta < diffrange / 500:  \n",
    "                        if self._number_of_histogram_bins:\n",
    "                            histogram_delta = diffrange / self._number_of_histogram_bins\n",
    "                        else:\n",
    "                            histogram_delta = diffrange / 500; \n",
    "                    self._histogram_delta_dict[feature_name] = histogram_delta\n",
    "                    num_of_histogram_bins = int(diffrange / histogram_delta) + 1\n",
    "                    self._num_of_histogram_bins_dict[feature_name] = num_of_histogram_bins\n",
    "                    sampling_points_for_feature = [valuerange[0] + histogram_delta * j for j in range(num_of_histogram_bins)]\n",
    "                    self._sampling_points_for_numeric_feature_dict[feature_name] = sampling_points_for_feature\n",
    "        if feature_name in self._numeric_features_valuerange_dict:\n",
    "            if self._feature_values_how_many_uniques_dict[feature_name] > self._symbolic_to_numeric_cardinality_threshold:\n",
    "                sampling_points_for_feature = self._sampling_points_for_numeric_feature_dict[feature_name]\n",
    "                counts_at_sampling_points = [0] * len(sampling_points_for_feature)\n",
    "                actual_values_for_feature = self._features_and_values_dict[feature_name]\n",
    "                actual_values_for_feature = list(filter(lambda x: x != 'NA', actual_values_for_feature))\n",
    "                for i in range(len(sampling_points_for_feature)):\n",
    "                    for j in range(len(actual_values_for_feature)):\n",
    "                        if abs(sampling_points_for_feature[i] - \n",
    "                           convert(actual_values_for_feature[j])) < (histogram_delta):\n",
    "                            counts_at_sampling_points[i] += 1\n",
    "                total_counts =  functools.reduce(lambda x,y:x+y, counts_at_sampling_points)\n",
    "                probs = [x / (1.0 * total_counts) for x in counts_at_sampling_points]\n",
    "                sum_probs =  functools.reduce(lambda x,y:x+y, probs)\n",
    "                bin_prob_dict = {sampling_points_for_feature[i] : probs[i] \n",
    "                               for i in range(len(sampling_points_for_feature))}\n",
    "                self._prob_distribution_numeric_features_dict[feature_name] = bin_prob_dict\n",
    "                values_for_feature = list(map(lambda x: feature_name + \"=\" + x, map(str, sampling_points_for_feature)))\n",
    "                for i in range(0, len(values_for_feature)):\n",
    "                    self._probability_cache[values_for_feature[i]] = probs[i]\n",
    "                if (value is not None) and (feature_and_value in self._probability_cache):\n",
    "                    return self._probability_cache[feature_and_value]\n",
    "                else:\n",
    "                    return 0\n",
    "            else:\n",
    "                # This section is for those numeric features treated symbolically:\n",
    "                values_for_feature = list(set(self._features_and_values_dict[feature_name]))\n",
    "                values_for_feature = list(filter(lambda x: x != 'NA', values_for_feature))\n",
    "                values_for_feature = list(map(lambda x: \"\".join([feature_name,\"=\",x]), map(str,values_for_feature)))\n",
    "                value_counts = [0] * len(values_for_feature)\n",
    "                for sample in sorted(self._training_data_dict.keys(), key = lambda x: sample_index(x) ):\n",
    "                    features_and_values = self._training_data_dict[sample]\n",
    "                    for i in range(0, len(values_for_feature)):\n",
    "                        for current_value in (features_and_values):\n",
    "                            if values_for_feature[i] == current_value:\n",
    "                                value_counts[i] += 1 \n",
    "                total_counts = functools.reduce(lambda x,y:x+y, value_counts)\n",
    "                if total_counts == 0:\n",
    "                    raise Exception('''PFV Something is wrong with your training file. '''\n",
    "                                    '''It contains no training samples for feature named %s ''' % feature_name)\n",
    "                probs = [x / (1.0 * total_counts) for x in value_counts]\n",
    "                for i in range(0, len(values_for_feature)):\n",
    "                    self._probability_cache[values_for_feature[i]] = probs[i]\n",
    "                if (value is not None) and (feature_and_value in self._probability_cache):\n",
    "                    return self._probability_cache[feature_and_value]\n",
    "                else:\n",
    "                    return 0\n",
    "        else:\n",
    "            # This section is only for purely symbolic features:  \n",
    "            values_for_feature = self._features_and_values_dict[feature_name]\n",
    "            values_for_feature = list(map(lambda x: feature_name + \"=\" + x, values_for_feature))\n",
    "            value_counts = [0] * len(values_for_feature)\n",
    "            for sample in sorted(self._training_data_dict.keys(), key = lambda x: sample_index(x) ):\n",
    "                features_and_values = self._training_data_dict[sample]\n",
    "                for i in range(0, len(values_for_feature)):\n",
    "                    for current_value in features_and_values:\n",
    "                        if values_for_feature[i] == current_value:\n",
    "                            value_counts[i] += 1 \n",
    "            for i in range(0, len(values_for_feature)):\n",
    "                self._probability_cache[values_for_feature[i]] = value_counts[i] / (1.0 * len(self._training_data_dict))\n",
    "            if (value is not None) and (feature_and_value in self._probability_cache):\n",
    "                return self._probability_cache[feature_and_value]\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "    def probability_of_feature_value_given_class(self,feature_name,feature_value,class_name):\n",
    "        feature_value = convert(feature_value)\n",
    "        histogram_delta = num_of_histogram_bins = valuerange = diffrange = None\n",
    "        if feature_name in self._sampling_points_for_numeric_feature_dict:\n",
    "            feature_value = closest_sampling_point(convert(feature_value), \\\n",
    "                       self._sampling_points_for_numeric_feature_dict[feature_name])\n",
    "        feature_value_class = \"\".join([feature_name,\"=\",str(feature_value),\"::\",class_name])\n",
    "        if feature_value_class in self._probability_cache:\n",
    "            return self._probability_cache[feature_value_class]\n",
    "        if feature_name in self._numeric_features_valuerange_dict:\n",
    "            if self._feature_values_how_many_uniques_dict[feature_name] > self._symbolic_to_numeric_cardinality_threshold:\n",
    "                histogram_delta = self._histogram_delta_dict[feature_name]\n",
    "                num_of_histogram_bins = self._num_of_histogram_bins_dict[feature_name]\n",
    "                valuerange = self._numeric_features_valuerange_dict[feature_name]\n",
    "                diffrange = valuerange[1] - valuerange[0]\n",
    "        samples_for_class = []\n",
    "        # Accumulate all samples names for the given class:\n",
    "        for sample_name in self._samples_class_label_dict:\n",
    "            if self._samples_class_label_dict[sample_name] == class_name:\n",
    "                samples_for_class.append(sample_name) \n",
    "        if feature_name in self._numeric_features_valuerange_dict:\n",
    "            if self._feature_values_how_many_uniques_dict[feature_name] > self._symbolic_to_numeric_cardinality_threshold:\n",
    "                sampling_points_for_feature = self._sampling_points_for_numeric_feature_dict[feature_name]\n",
    "                counts_at_sampling_points = [0] * len(sampling_points_for_feature)\n",
    "                actual_feature_values_for_samples_in_class = []\n",
    "                for sample in samples_for_class:\n",
    "                    for feature_and_value in self._training_data_dict[sample]:\n",
    "                        pattern = r'(.+)=(.+)'\n",
    "                        m = re.search(pattern, feature_and_value)\n",
    "                        feature,value = m.group(1),m.group(2)\n",
    "                        if feature == feature_name and value != 'NA':\n",
    "                            actual_feature_values_for_samples_in_class.append(convert(value))\n",
    "                for i in range(len(sampling_points_for_feature)):\n",
    "                    for j in range(len(actual_feature_values_for_samples_in_class)):\n",
    "                        if abs(sampling_points_for_feature[i] - \n",
    "                     actual_feature_values_for_samples_in_class[j]) < histogram_delta:\n",
    "                            counts_at_sampling_points[i] += 1\n",
    "                total_counts =  functools.reduce(lambda x,y:x+y, counts_at_sampling_points)\n",
    "                probs = [x / (1.0 * total_counts) for x in counts_at_sampling_points]\n",
    "                if total_counts == 0:\n",
    "                    raise Exception('''PFVC1 Something is wrong with your training file. It contains no training '''\n",
    "                                    '''samples for Class %s and Feature %s''' % class_name, feature_name)\n",
    "                values_for_feature_and_class = list(map(lambda x: feature_name + \"=\" + x + \n",
    "                                \"::\" + class_name, map(str, sampling_points_for_feature)))\n",
    "                for i in range(0, len(values_for_feature_and_class)):\n",
    "                    self._probability_cache[values_for_feature_and_class[i]] = probs[i]\n",
    "                if feature_value_class in self._probability_cache:\n",
    "                    return self._probability_cache[feature_value_class]\n",
    "                else:\n",
    "                    return 0\n",
    "            else:\n",
    "                # We now take care of numeric features with a small number of unique values\n",
    "                values_for_feature = list(set(self._features_and_values_dict[feature_name]))\n",
    "                values_for_feature = list(filter(lambda x: x != 'NA', values_for_feature))\n",
    "                values_for_feature =list(map(lambda x: \"\".join([feature_name,\"=\",x]), map(str,map(convert, values_for_feature))))\n",
    "                value_counts = [0] * len(values_for_feature)\n",
    "                for sample in samples_for_class:\n",
    "                    features_and_values = self._training_data_dict[sample]\n",
    "                    for i in range(0, len(values_for_feature)):\n",
    "                        for current_value in (features_and_values):\n",
    "                            if values_for_feature[i] == current_value:\n",
    "                                value_counts[i] += 1 \n",
    "                total_count = functools.reduce(lambda x,y:x+y, value_counts)\n",
    "                if total_count == 0:\n",
    "                    raise Exception('''PFVC2 Something is wrong with your training file. It contains no training samples '''\n",
    "                                    '''for Class %s and Feature %s''' % (class_name, feature_name))\n",
    "                # We normalize by total_count because the probabilities are\n",
    "                # conditioned on a given class\n",
    "                for i in range(len(values_for_feature)):\n",
    "                    feature_and_value_and_class =  values_for_feature[i] + \"::\" + class_name\n",
    "                    self._probability_cache[feature_and_value_and_class] = value_counts[i] / (1.0 * total_count)\n",
    "                if feature_value_class in self._probability_cache:\n",
    "                    return self._probability_cache[feature_value_class]\n",
    "                else:\n",
    "                    return 0\n",
    "        else:\n",
    "            # This section is for purely symbolic features\n",
    "            values_for_feature = list(set(self._features_and_values_dict[feature_name]))\n",
    "            values_for_feature = \\\n",
    "                        list(map(lambda x: \"\".join([feature_name,\"=\",x]), map(str,values_for_feature)))\n",
    "            value_counts = [0] * len(values_for_feature)\n",
    "            for sample in samples_for_class:\n",
    "                features_and_values = self._training_data_dict[sample]\n",
    "                for i in range(len(values_for_feature)):\n",
    "                    for current_value in (features_and_values):\n",
    "                        if values_for_feature[i] == current_value:\n",
    "                            value_counts[i] += 1 \n",
    "            total_count = functools.reduce(lambda x,y:x+y, value_counts)\n",
    "            if total_count == 0:\n",
    "                raise Exception('''PFVC3 Something is wrong with your training file. It contains no training samples '''\n",
    "                              '''for Class %s and Feature %s''' % (class_name, feature_name))\n",
    "            # We normalize by total_count because the probabilities are\n",
    "            # conditioned on a given class\n",
    "            for i in range(0, len(values_for_feature)):\n",
    "                feature_and_value_for_class = \"\".join([values_for_feature[i],\"::\",class_name])\n",
    "                self._probability_cache[feature_and_value_for_class] = value_counts[i] / (1.0 * total_count)\n",
    "            feature_and_value_and_class = \"\".join([feature_name,\"=\", feature_value,\"::\",class_name])\n",
    "            if feature_and_value_and_class in self._probability_cache:\n",
    "                return self._probability_cache[feature_and_value_and_class]\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "    def probability_of_feature_less_than_threshold(self, feature_name, threshold):\n",
    "        threshold = convert(threshold)\n",
    "        feature_threshold_combo = feature_name + '<' + str(threshold)\n",
    "        if feature_threshold_combo in self._probability_cache:\n",
    "            return self._probability_cache[feature_threshold_combo]\n",
    "        all_values = list(filter(lambda x: x != 'NA', self._features_and_values_dict[feature_name]))\n",
    "        all_values_less_than_threshold = list(filter(lambda x: x <= threshold, all_values))\n",
    "        probability = 1.0 * len(all_values_less_than_threshold) / len(all_values)\n",
    "        self._probability_cache[feature_threshold_combo] = probability\n",
    "        return probability\n",
    "\n",
    "    def probability_of_feature_less_than_threshold_given_class(self, feature_name, threshold, class_name):\n",
    "        threshold = convert(threshold)\n",
    "        feature_threshold_class_combo = \"\".join([feature_name,'<',str(threshold),\"::\",class_name])\n",
    "        if feature_threshold_class_combo in self._probability_cache:\n",
    "            return self._probability_cache[feature_threshold_class_combo]\n",
    "        data_samples_for_class = []\n",
    "        # Accumulate all samples names for the given class:\n",
    "        for sample_name in self._samples_class_label_dict:\n",
    "            if self._samples_class_label_dict[sample_name] == class_name:\n",
    "                data_samples_for_class.append(sample_name) \n",
    "        actual_feature_values_for_samples_in_class = []\n",
    "        for sample in data_samples_for_class:\n",
    "            for feature_and_value in self._training_data_dict[sample]:\n",
    "                pattern = r'(.+)=(.+)'\n",
    "                m = re.search(pattern, feature_and_value)\n",
    "                feature,value = m.group(1),m.group(2)\n",
    "                if feature == feature_name and value != 'NA':\n",
    "                    actual_feature_values_for_samples_in_class.append(convert(value))\n",
    "        actual_points_for_feature_less_than_threshold = list(filter(lambda x: x <= threshold, \n",
    "                                          actual_feature_values_for_samples_in_class))\n",
    "        # The conditional in the assignment shown below introduced in Version 3.2.1:\n",
    "        probability = ((1.0 * len(actual_points_for_feature_less_than_threshold)) / len(actual_feature_values_for_samples_in_class)) if len(actual_feature_values_for_samples_in_class) > 0 else 0.0\n",
    "        self._probability_cache[feature_threshold_class_combo] = probability\n",
    "        return probability\n",
    "\n",
    "    def probability_of_a_sequence_of_features_and_values_or_thresholds(self,array_of_features_and_values_or_thresholds):\n",
    "        '''\n",
    "        This method requires that all truly numeric types only be expressed as '<' or '>'\n",
    "        constructs in the array of branch features and thresholds\n",
    "        '''\n",
    "        if len(array_of_features_and_values_or_thresholds) == 0: return\n",
    "        sequence = \":\".join(array_of_features_and_values_or_thresholds)\n",
    "        if sequence in self._probability_cache:\n",
    "            return self._probability_cache[sequence]\n",
    "        probability = None\n",
    "        pattern1 = r'(.+)=(.+)'\n",
    "        pattern2 = r'(.+)<(.+)'\n",
    "        pattern3 = r'(.+)>(.+)'\n",
    "        true_numeric_types = []        \n",
    "        true_numeric_types_feature_names = []\n",
    "        symbolic_types = []\n",
    "        symbolic_types_feature_names = []\n",
    "        for item in array_of_features_and_values_or_thresholds:\n",
    "            if re.search(pattern2, item):\n",
    "                true_numeric_types.append(item)\n",
    "                m = re.search(pattern2, item)\n",
    "                feature,value = m.group(1),m.group(2)\n",
    "                true_numeric_types_feature_names.append(feature)\n",
    "            elif re.search(pattern3, item): \n",
    "                true_numeric_types.append(item)\n",
    "                m = re.search(pattern3, item)\n",
    "                feature,value = m.group(1),m.group(2)\n",
    "                true_numeric_types_feature_names.append(feature)\n",
    "            else:\n",
    "                symbolic_types.append(item) \n",
    "                m = re.search(pattern1, item)\n",
    "                feature,value = m.group(1),m.group(2)\n",
    "                symbolic_types_feature_names.append(feature)\n",
    "        true_numeric_types_feature_names = list(set(true_numeric_types_feature_names))\n",
    "        symbolic_types_feature_names = list(set(symbolic_types_feature_names))\n",
    "        bounded_intervals_numeric_types = self.find_bounded_intervals_for_numeric_features(true_numeric_types)\n",
    "        # Calculate the upper and the lower bounds to be used when searching for the best\n",
    "        # threshold for each of the numeric features that are in play at the current node:\n",
    "        upperbound = {feature : None for feature in true_numeric_types_feature_names}\n",
    "        lowerbound = {feature : None for feature in true_numeric_types_feature_names}\n",
    "        for item in bounded_intervals_numeric_types:\n",
    "            if item[1] == '>':\n",
    "                lowerbound[item[0]] = float(item[2])\n",
    "            else:\n",
    "                upperbound[item[0]] = float(item[2])\n",
    "        for feature_name in true_numeric_types_feature_names:\n",
    "            if lowerbound[feature_name] is not None and upperbound[feature_name] is not None \\\n",
    "                          and upperbound[feature_name] <= lowerbound[feature_name]:\n",
    "                return 0\n",
    "            elif lowerbound[feature_name] is not None and upperbound[feature_name] is not None:\n",
    "                if not probability:\n",
    "                    probability = self.probability_of_feature_less_than_threshold(feature_name, upperbound[feature_name]) - \\\n",
    "                          self.probability_of_feature_less_than_threshold(feature_name, lowerbound[feature_name])\n",
    "                else:\n",
    "                    probability *= (self.probability_of_feature_less_than_threshold(feature_name, upperbound[feature_name]) - \\\n",
    "                          self.probability_of_feature_less_than_threshold(feature_name, lowerbound[feature_name]))\n",
    "            elif upperbound[feature_name] is not None and lowerbound[feature_name] is None:\n",
    "                if not probability:\n",
    "                    probability = self.probability_of_feature_less_than_threshold(feature_name, upperbound[feature_name])\n",
    "                else:\n",
    "                    probability *= self.probability_of_feature_less_than_threshold(feature_name, upperbound[feature_name])\n",
    "            elif lowerbound[feature_name] is not None and upperbound[feature_name] is None:\n",
    "                if not probability:\n",
    "                    probability = 1.0 -self.probability_of_feature_less_than_threshold(feature_name, lowerbound[feature_name])\n",
    "                else:\n",
    "                    probability *= (1.0 - self.probability_of_feature_less_than_threshold(feature_name,\n",
    "                                                                                          lowerbound[feature_name]))\n",
    "            else:\n",
    "                raise SyntaxError(\"Ill formatted call to 'probability_of_sequence' method\")\n",
    "        for feature_and_value in symbolic_types:\n",
    "            if re.search(pattern1, feature_and_value):      \n",
    "                m = re.search(pattern1, feature_and_value)\n",
    "                feature,value = m.group(1),m.group(2)\n",
    "                if not probability:\n",
    "                    probability = self.probability_of_feature_value(feature, value)\n",
    "                else:\n",
    "                    probability *= self.probability_of_feature_value(feature, value)\n",
    "        self._probability_cache[sequence] = probability\n",
    "        return probability\n",
    "\n",
    "    def probability_of_a_sequence_of_features_and_values_or_thresholds_given_class(self, \n",
    "                                              array_of_features_and_values_or_thresholds, class_name):\n",
    "        '''\n",
    "        This method requires that all truly numeric types only be expressed as '<' or '>'\n",
    "        constructs in the array of branch features and thresholds\n",
    "        '''\n",
    "        if len(array_of_features_and_values_or_thresholds) == 0: return\n",
    "        sequence = \":\".join(array_of_features_and_values_or_thresholds)\n",
    "        sequence_with_class = sequence + \"::\" + class_name\n",
    "        if sequence_with_class in self._probability_cache:\n",
    "            return self._probability_cache[sequence_with_class]\n",
    "        probability = None\n",
    "        pattern1 = r'(.+)=(.+)'\n",
    "        pattern2 = r'(.+)<(.+)'\n",
    "        pattern3 = r'(.+)>(.+)'\n",
    "        true_numeric_types = []        \n",
    "        true_numeric_types_feature_names = []        \n",
    "        symbolic_types = []\n",
    "        symbolic_types_feature_names = []\n",
    "        for item in array_of_features_and_values_or_thresholds:\n",
    "            if re.search(pattern2, item):\n",
    "                true_numeric_types.append(item)\n",
    "                m = re.search(pattern2, item)\n",
    "                feature,value = m.group(1),m.group(2)\n",
    "                true_numeric_types_feature_names.append(feature)\n",
    "            elif re.search(pattern3, item): \n",
    "                true_numeric_types.append(item)\n",
    "                m = re.search(pattern3, item)\n",
    "                feature,value = m.group(1),m.group(2)\n",
    "                true_numeric_types_feature_names.append(feature)\n",
    "            else:\n",
    "                symbolic_types.append(item) \n",
    "                m = re.search(pattern1, item)\n",
    "                feature,value = m.group(1),m.group(2)\n",
    "                symbolic_types_feature_names.append(feature)\n",
    "        true_numeric_types_feature_names = list(set(true_numeric_types_feature_names))\n",
    "        symbolic_types_feature_names = list(set(symbolic_types_feature_names))\n",
    "        bounded_intervals_numeric_types = self.find_bounded_intervals_for_numeric_features(true_numeric_types)\n",
    "        # Calculate the upper and the lower bounds to be used when searching for the best\n",
    "        # threshold for each of the numeric features that are in play at the current node:\n",
    "        upperbound = {feature : None for feature in true_numeric_types_feature_names}\n",
    "        lowerbound = {feature : None for feature in true_numeric_types_feature_names}\n",
    "        for item in bounded_intervals_numeric_types:\n",
    "            if item[1] == '>':\n",
    "                lowerbound[item[0]] = float(item[2])\n",
    "            else:\n",
    "                upperbound[item[0]] = float(item[2])\n",
    "        for feature_name in true_numeric_types_feature_names:\n",
    "            if lowerbound[feature_name] is not None and upperbound[feature_name] is not None \\\n",
    "                          and upperbound[feature_name] <= lowerbound[feature_name]:\n",
    "                return 0\n",
    "            elif lowerbound[feature_name] is not None and upperbound[feature_name] is not None:\n",
    "                if not probability:\n",
    "                    probability = self.probability_of_feature_less_than_threshold_given_class(feature_name, \n",
    "                                                upperbound[feature_name], class_name) - \\\n",
    "                          self.probability_of_feature_less_than_threshold_given_class(feature_name, \n",
    "                                                lowerbound[feature_name], class_name)\n",
    "                else:\n",
    "                    probability *= (self.probability_of_feature_less_than_threshold_given_class(feature_name, \n",
    "                                  upperbound[feature_name], class_name) - \\\n",
    "                          self.probability_of_feature_less_than_threshold_given_class(feature_name, \n",
    "                                                        lowerbound[feature_name], class_name))\n",
    "            elif upperbound[feature_name] is not None and lowerbound[feature_name] is None:\n",
    "                if not probability:\n",
    "                    probability = self.probability_of_feature_less_than_threshold_given_class(feature_name, \n",
    "                                                         upperbound[feature_name], class_name)\n",
    "                else:\n",
    "                    probability *= self.probability_of_feature_less_than_threshold_given_class(feature_name, \n",
    "                                                         upperbound[feature_name], class_name)\n",
    "            elif lowerbound[feature_name] is not None and upperbound[feature_name] is None:\n",
    "                if not probability:\n",
    "                    probability = 1.0 - \\\n",
    "                          self.probability_of_feature_less_than_threshold_given_class(feature_name, \n",
    "                                                         lowerbound[feature_name], class_name)\n",
    "                else:\n",
    "                    probability *= (1.0 - \n",
    "                          self.probability_of_feature_less_than_threshold_given_class(feature_name, \n",
    "                                                         lowerbound[feature_name], class_name))\n",
    "            else:\n",
    "                raise SyntaxError(\"Ill formatted call to 'probability of sequence with class' method\")\n",
    "        for feature_and_value in symbolic_types:\n",
    "            if re.search(pattern1, feature_and_value):      \n",
    "                m = re.search(pattern1, feature_and_value)\n",
    "                feature,value = m.group(1),m.group(2)\n",
    "                if not probability:\n",
    "                    probability = self.probability_of_feature_value_given_class(feature, value, class_name)\n",
    "                else:\n",
    "                    probability *= self.probability_of_feature_value_given_class(feature, value, class_name)\n",
    "        self._probability_cache[sequence_with_class] = probability\n",
    "        return probability\n",
    "\n",
    "    def probability_of_a_class_given_sequence_of_features_and_values_or_thresholds(self, \n",
    "                                  class_name, array_of_features_and_values_or_thresholds):\n",
    "        sequence = \":\".join(array_of_features_and_values_or_thresholds)\n",
    "        class_and_sequence = \"\".join([class_name, \"::\", sequence])\n",
    "        if class_and_sequence in self._probability_cache:\n",
    "            return self._probability_cache[class_and_sequence]\n",
    "        array_of_class_probabilities = [0] * len(self._class_names)\n",
    "        for i in range(len(self._class_names)):\n",
    "            class_name = self._class_names[i]\n",
    "            prob = self.probability_of_a_sequence_of_features_and_values_or_thresholds_given_class(\n",
    "                                     array_of_features_and_values_or_thresholds, class_name) \n",
    "            if prob < 0.000001:\n",
    "                array_of_class_probabilities[i] = 0.0\n",
    "                continue\n",
    "            prob_of_feature_sequence = self.probability_of_a_sequence_of_features_and_values_or_thresholds(\n",
    "                                                      array_of_features_and_values_or_thresholds)\n",
    "            ##Commented out in 2.2.4\n",
    "            #if not prob_of_feature_sequence: \n",
    "            #    sys.exit('''PCS Something is wrong with your sequence of feature values and thresholds in '''\n",
    "            #       '''probability_of_a_class_given_sequence_of_features_and_values_or_thresholds()''')\n",
    "            prior = self._class_priors_dict[self._class_names[i]] \n",
    "            if prob_of_feature_sequence: \n",
    "                array_of_class_probabilities[i] = prob * prior / prob_of_feature_sequence\n",
    "            else:\n",
    "                array_of_class_probabilities[i] = prior\n",
    "        sum_probability = functools.reduce(lambda x,y:x+y, array_of_class_probabilities)\n",
    "        if sum_probability == 0:\n",
    "            array_of_class_probabilities = [1.0/len(self._class_names)] * len(self._class_names)\n",
    "        else:\n",
    "            array_of_class_probabilities = \\\n",
    "                             list(map(lambda x: x / sum_probability, array_of_class_probabilities))\n",
    "        for i in range(len(self._class_names)):\n",
    "            this_class_and_sequence = \"\".join([self._class_names[i], \"::\", sequence])\n",
    "            self._probability_cache[this_class_and_sequence] = array_of_class_probabilities[i]\n",
    "        return self._probability_cache[class_and_sequence]\n",
    "\n",
    "    \n",
    "#---------------------------------  Class Based Utilities  ------------------------------------\n",
    "\n",
    "    def determine_data_condition(self):\n",
    "        '''\n",
    "        This method estimates the worst-case fan-out of the decision tree taking into\n",
    "        account the number of values (and therefore the number of branches emanating\n",
    "        from a node) for the symbolic features.\n",
    "        '''\n",
    "        num_of_features = len(self._feature_names)\n",
    "        values = []\n",
    "        for feature in self._features_and_unique_values_dict:  \n",
    "            if feature not in self._numeric_features_valuerange_dict:\n",
    "                values.append(self._features_and_unique_values_dict[feature])\n",
    "        # return if no symbolic features found:\n",
    "        if not values: return\n",
    "        print(\"Number of features: \" + str(num_of_features))\n",
    "        max_num_values = max(list(map(len, values)))\n",
    "        print(\"Largest number of values for symbolic features is: \" + str(max_num_values))\n",
    "        estimated_number_of_nodes = max_num_values ** num_of_features\n",
    "        print('''\\nWORST CASE SCENARIO: The decision tree COULD have as many as %s '''\n",
    "              ''' nodes. The exact number of nodes created depends critically on '''\n",
    "              '''the entropy_threshold used for node expansion (the default value '''\n",
    "              '''for this threshold is 0.01) and on the value set for max_depth_desired '''\n",
    "              '''for the depth of the tree\\n''' % estimated_number_of_nodes)\n",
    "        if estimated_number_of_nodes > 10000:\n",
    "            print('''THIS IS WAY TOO MANY NODES. Consider using a relatively '''\n",
    "                  '''large value for entropy_threshold and/or a small value for '''\n",
    "                  '''for max_depth_desired to reduce the number of nodes created''')\n",
    "            ans = None\n",
    "            if sys.version_info[0] == 3:\n",
    "                ans = input(\"\\nDo you wish to continue? Enter 'y' if yes:  \")\n",
    "            else:\n",
    "                ans = raw_input(\"\\nDo you wish to continue? Enter 'y' if yes:  \")\n",
    "            ans = ans.strip()\n",
    "            if ans != 'y':\n",
    "                sys.exit(0)\n",
    " \n",
    "    def _check_names_used(self, features_and_values_test_data):\n",
    "        '''\n",
    "        This method is used to verify that you used legal feature names in the test\n",
    "        sample that you want to classify with the decision tree.\n",
    "        '''\n",
    "        for feature_and_value in features_and_values_test_data:\n",
    "            pattern = r'(\\S+)\\s*=\\s*(\\S+)'\n",
    "            m = re.search(pattern, feature_and_value)\n",
    "            feature,value = m.group(1),m.group(2)\n",
    "            if feature is None or value is None:\n",
    "                raise Exception(\"Your test data has formatting error\")\n",
    "            if feature not in self._feature_names:\n",
    "                return 0\n",
    "        return 1\n",
    "\n",
    "    def get_class_names(self):\n",
    "        return self._class_names\n",
    "\n",
    "    def find_bounded_intervals_for_numeric_features(self, arr):\n",
    "        '''\n",
    "        Given a list of branch attributes for the numeric features of the form, say,\n",
    "        ['g2<1','g2<2','g2<3','age>34','age>36','age>37'], this method returns the\n",
    "        smallest list that is relevant for the purpose of calculating the\n",
    "        probabilities.  To explain, the probability that the feature `g2' is less\n",
    "        than 1 AND, at the same time, less than 2, AND, at the same time, less than\n",
    "        3, is the same as the probability that the feature less than 1. Similarly,\n",
    "        the probability that 'age' is greater than 34 and also greater than 37 is the\n",
    "        same as `age' being greater than 37.\n",
    "        '''       \n",
    "        features = self._feature_names\n",
    "        arr1 = list(map(lambda x: re.split(r'(>|<)', x, 0), arr))\n",
    "        # make a separate list for each feature name:                                    \n",
    "        arr3 = list(filter(lambda x: len(x)>0, [list(filter(lambda x: x[0]==y, arr1)) for y in features]))\n",
    "        # Sort each list so that '<' entries occur before '>' entries:                   \n",
    "        arr4 = [sorted(li, key=lambda x: x[1]) for li in arr3]\n",
    "        arr5 = [[list(filter(lambda x: x[1]==y, alist))] for alist in arr4 for y in ['<','>']]\n",
    "        arr6 = []\n",
    "        for i in range(len(arr5)):\n",
    "            arr6 += [sorted(li, key=lambda x: float(x[2])) for li in arr5[i]]\n",
    "        arr7 = list(itertools.chain(*arr6))\n",
    "        arr8 = list(filter(lambda x: len(x)>0, [list(filter(lambda x: x[0]==y, arr7)) for y in features]))\n",
    "        arr9 = []\n",
    "        for alist in arr8:\n",
    "            newalist = []\n",
    "            if alist[0][1] == '<':\n",
    "                newalist.append(alist[0])\n",
    "            else:\n",
    "                newalist.append(alist[-1])\n",
    "            if alist[0][1] != alist[-1][1]:\n",
    "                newalist.append(alist[-1])\n",
    "            arr9.append(newalist)\n",
    "        arr10 = list(itertools.chain(*arr9))\n",
    "        return arr10\n",
    "\n",
    "\n",
    "#-------------------------------------------  Class DTNode   ------------------------------------------\n",
    "\n",
    "class DTNode(object):\n",
    "    '''\n",
    "    The nodes of a decision tree are instances of this class:\n",
    "    '''\n",
    "    def __init__(self, feature, entropy, class_probabilities, branch_features_and_values_or_thresholds, dt,\n",
    "                          root_or_not = None):\n",
    "        if root_or_not == 'root':\n",
    "             dt.nodes_created = -1 \n",
    "             dt.class_names = None\n",
    "        self._dt = dt\n",
    "        self._serial_number               = self.get_next_serial_num()\n",
    "        self._feature                     = feature\n",
    "        self._node_creation_entropy       = entropy\n",
    "        self._class_probabilities = class_probabilities\n",
    "        self._branch_features_and_values_or_thresholds = branch_features_and_values_or_thresholds\n",
    "        self._linked_to = []\n",
    "\n",
    "    def how_many_nodes(self):\n",
    "        return self._dt.nodes_created + 1\n",
    "\n",
    "    def set_class_names(self, class_names_list):\n",
    "        self._dt.class_names = class_names_list\n",
    "\n",
    "    def get_class_names(self):\n",
    "        return self._dt.class_names\n",
    "\n",
    "    def set_node_creation_entropy(self, entropy):\n",
    "        self._node_creation_entropy = entropy\n",
    "\n",
    "    def get_next_serial_num(self):\n",
    "        self._dt.nodes_created += 1\n",
    "        return self._dt.nodes_created\n",
    "\n",
    "    def get_serial_num(self):\n",
    "        return self._serial_number\n",
    "\n",
    "    def get_feature(self):\n",
    "        '''\n",
    "        Returns the feature test at the current node\n",
    "        '''\n",
    "        return self._feature\n",
    "\n",
    "    def set_feature(self, feature):\n",
    "        self._feature = feature\n",
    "\n",
    "    def get_node_entropy(self):\n",
    "        return self._node_creation_entropy\n",
    "\n",
    "    def get_class_probabilities(self):\n",
    "        return self._class_probabilities\n",
    "\n",
    "    def get_branch_features_and_values_or_thresholds(self):\n",
    "        return self._branch_features_and_values_or_thresholds\n",
    "\n",
    "    def get_children(self):\n",
    "        return self._linked_to\n",
    "\n",
    "    def add_child_link(self, new_node):\n",
    "        self._linked_to.append(new_node)                  \n",
    "\n",
    "    def delete_all_links(self):\n",
    "        self._linked_to = None\n",
    "\n",
    "    def display_node(self):\n",
    "        feature_at_node = self.get_feature() or \" \"\n",
    "        node_creation_entropy_at_node = self.get_node_entropy()\n",
    "        print_node_creation_entropy_at_node = \"%.3f\" % node_creation_entropy_at_node\n",
    "        class_probabilities = self.get_class_probabilities()\n",
    "        class_probabilities_for_display = [\"%0.3f\" % x for x in class_probabilities]\n",
    "        serial_num = self.get_serial_num()\n",
    "        branch_features_and_values_or_thresholds = self.get_branch_features_and_values_or_thresholds()\n",
    "        print(\"\\n\\nNODE \" + str(serial_num) + \n",
    "              \":\\n   Branch features and values to this node: \" + \n",
    "              str(branch_features_and_values_or_thresholds) + \n",
    "              \"\\n   Class probabilities at current node: \" + \n",
    "              str(class_probabilities_for_display) + \n",
    "              \"\\n   Entropy at current node: \" + \n",
    "              print_node_creation_entropy_at_node + \n",
    "              \"\\n   Best feature test at current node: \" + feature_at_node + \"\\n\\n\")\n",
    "\n",
    "    def display_decision_tree(self, offset):\n",
    "        serial_num = self.get_serial_num()\n",
    "        if len(self.get_children()) > 0:\n",
    "            feature_at_node = self.get_feature() or \" \"\n",
    "            node_creation_entropy_at_node = self.get_node_entropy()\n",
    "            print_node_creation_entropy_at_node = \"%.3f\" % node_creation_entropy_at_node\n",
    "            branch_features_and_values_or_thresholds = self.get_branch_features_and_values_or_thresholds()\n",
    "            class_probabilities = self.get_class_probabilities()\n",
    "            print_class_probabilities = [\"%.3f\" % x for x in class_probabilities]\n",
    "            print_class_probabilities_with_class = [self.get_class_names()[i] + \" => \" + \n",
    "                         print_class_probabilities[i] for i in range(len(self.get_class_names()))]\n",
    "            print(\"NODE \" + str(serial_num) + \":  \" + offset +  \"BRANCH TESTS TO NODE: \" +\n",
    "                                           str(branch_features_and_values_or_thresholds))\n",
    "            second_line_offset = offset + \" \" * (8 + len(str(serial_num)))\n",
    "            print(second_line_offset +  \"Decision Feature: \" + \n",
    "                  feature_at_node + \"   Node Creation Entropy: \" + print_node_creation_entropy_at_node +  \n",
    "                  \"   Class Probs: \" + str(print_class_probabilities_with_class) + \"\\n\")\n",
    "            offset += \"   \"\n",
    "            for child in self.get_children():\n",
    "                child.display_decision_tree(offset)\n",
    "        else:\n",
    "            node_creation_entropy_at_node = self.get_node_entropy()\n",
    "            print_node_creation_entropy_at_node = \"%.3f\" % node_creation_entropy_at_node\n",
    "            branch_features_and_values_or_thresholds = self.get_branch_features_and_values_or_thresholds()\n",
    "            class_probabilities = self.get_class_probabilities()\n",
    "            print_class_probabilities = [\"%.3f\" % x for x in class_probabilities]\n",
    "            print_class_probabilities_with_class = [self.get_class_names()[i] + \" => \" + \n",
    "                   print_class_probabilities[i] for i in range(len(self.get_class_names()))]\n",
    "            print(\"NODE \" + str(serial_num) + \":  \" + offset +  \"BRANCH TESTS TO LEAF NODE: \" +\n",
    "                                           str(branch_features_and_values_or_thresholds))\n",
    "            second_line_offset = offset + \" \" * (8 + len(str(serial_num)))\n",
    "            print(second_line_offset + \"Node Creation Entropy: \" + print_node_creation_entropy_at_node +  \n",
    "                  \"   Class Probs: \" + str(print_class_probabilities_with_class) + \"\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "#-----------------------------  Evaluate Quality of Training Data  ----------------------------\n",
    "\n",
    "class EvalTrainingData(DecisionTree):\n",
    "    def __init__(self, *args, **kwargs ):\n",
    "        DecisionTree.__init__(self, *args, **kwargs)\n",
    "\n",
    "    def evaluate_training_data(self):\n",
    "        evaldebug = 0\n",
    "        if not self._training_datafile.endswith('.csv'):\n",
    "            raise SyntaxError('''The data evaluation function in the module can only be used when your '''\n",
    "                              '''training data is in a CSV file''')\n",
    "        print('''\\nWill run a 10-fold cross-validation test on your training data to test its '''\n",
    "              '''class-discriminatory power:''')\n",
    "        all_training_data = self._training_data_dict\n",
    "        all_sample_names = sorted(all_training_data.keys(), key = lambda x: sample_index(x))\n",
    "        fold_size = int(0.1 * len(all_training_data))\n",
    "        confusion_matrix = {class_name : {class_name : 0 for class_name in self._class_names} \\\n",
    "                                                                  for class_name in self._class_names}\n",
    "        for fold_index in range(10):\n",
    "            print(\"\\nStarting the iteration indexed %d of the 10-fold cross-validation test\" % fold_index)\n",
    "            testing_samples = all_sample_names[fold_size * fold_index : fold_size * (fold_index+1)]\n",
    "            training_samples = all_sample_names[0 : fold_size * fold_index] + \\\n",
    "                                                      all_sample_names[fold_size * (fold_index+1):] \n",
    "            testing_data = { x : all_training_data[x] for x in testing_samples }\n",
    "            training_data = { x : all_training_data[x] for x in training_samples }\n",
    "            trainingDT = DecisionTree('evalmode')\n",
    "            trainingDT._training_data_dict = training_data\n",
    "            trainingDT._class_names = self._class_names\n",
    "            trainingDT._feature_names = self._feature_names\n",
    "            trainingDT._entropy_threshold = self._entropy_threshold\n",
    "            trainingDT._max_depth_desired = self._max_depth_desired\n",
    "            trainingDT._symbolic_to_numeric_cardinality_threshold =  self._symbolic_to_numeric_cardinality_threshold\n",
    "            trainingDT._samples_class_label_dict = {sample_name : self._samples_class_label_dict[sample_name] \n",
    "                                                           for sample_name in training_samples}\n",
    "            trainingDT._features_and_values_dict = {feature : [] for feature in self._features_and_values_dict}\n",
    "            pattern = r'(\\S+)\\s*=\\s*(\\S+)'        \n",
    "            for item in sorted(trainingDT._training_data_dict.items(), key = lambda x: sample_index(x[0])):\n",
    "                for feature_and_value in item[1]:\n",
    "                    m = re.search(pattern, feature_and_value)\n",
    "                    feature,value = m.group(1),m.group(2)\n",
    "                    if value != 'NA':\n",
    "                        trainingDT._features_and_values_dict[feature].append(convert(value))\n",
    "            trainingDT._features_and_unique_values_dict = {feature : \n",
    "                                      sorted(list(set(trainingDT._features_and_values_dict[feature]))) for \n",
    "                                                            feature in trainingDT._features_and_values_dict}\n",
    "            trainingDT._numeric_features_valuerange_dict = {feature : [] \n",
    "                                                         for feature in self._numeric_features_valuerange_dict}\n",
    "            trainingDT._numeric_features_valuerange_dict = {feature : \n",
    "                                      [min(trainingDT._features_and_unique_values_dict[feature]), \n",
    "                                       max(trainingDT._features_and_unique_values_dict[feature])] \n",
    "                                                         for feature in self._numeric_features_valuerange_dict}\n",
    "            if evaldebug:\n",
    "                print(\"\\n\\nprinting samples in the testing set: \" + str(testing_samples))            \n",
    "                print(\"\\n\\nPrinting features and their values in the training set:\\n\")\n",
    "                for item in sorted(trainingDT._features_and_values_dict.items()):\n",
    "                    print(item[0]  + \"  =>  \"  + str(item[1]))\n",
    "                print(\"\\n\\nPrinting unique values for features:\\n\")\n",
    "                for item in sorted(trainingDT._features_and_unique_values_dict.items()):\n",
    "                    print(item[0]  + \"  =>  \"  + str(item[1]))\n",
    "                print(\"\\n\\nPrinting unique value ranges for features:\\n\")\n",
    "                for item in sorted(trainingDT._numeric_features_valuerange_dict.items()):\n",
    "                    print(item[0]  + \"  =>  \"  + str(item[1]))\n",
    "            trainingDT._feature_values_how_many_uniques_dict = {feature : [] \n",
    "                                                    for  feature in self._features_and_unique_values_dict}\n",
    "            trainingDT._feature_values_how_many_uniques_dict = {feature : \n",
    "                                     len(trainingDT._features_and_unique_values_dict[feature]) \n",
    "                                                    for  feature in self._features_and_unique_values_dict}\n",
    "            if evaldebug: trainingDT._debug2 = 1\n",
    "            trainingDT.calculate_first_order_probabilities()\n",
    "            trainingDT.calculate_class_priors()\n",
    "            root_node = trainingDT.construct_decision_tree_classifier()\n",
    "            if evaldebug:\n",
    "                root_node.display_decision_tree(\"     \")\n",
    "            for test_sample_name in testing_samples:\n",
    "                test_sample_data = all_training_data[test_sample_name]\n",
    "                if evaldebug: \n",
    "                    print(\"original data in test sample:\", str(test_sample_data))  \n",
    "                test_sample_data = [x for x in test_sample_data if not x.endswith('=NA')]\n",
    "                if evaldebug: \n",
    "                    print(\"data in test sample:\", str(test_sample_data))  \n",
    "                classification = trainingDT.classify(root_node, test_sample_data)\n",
    "                solution_path = classification['solution_path']                                  \n",
    "                del classification['solution_path']                                              \n",
    "                which_classes = list( classification.keys() )                                    \n",
    "                which_classes = sorted(which_classes, key=lambda x: classification[x], reverse=True)\n",
    "                most_likely_class_label = which_classes[0]\n",
    "                if evaldebug:\n",
    "                    print(\"\\nClassification:\\n\")                                                     \n",
    "                    print(\"     \"  + str.ljust(\"class name\", 30) + \"probability\")                    \n",
    "                    print(\"     ----------                    -----------\")                          \n",
    "                    for which_class in which_classes:                                                \n",
    "                        if which_class is not 'solution_path':                                       \n",
    "                            print(\"     \"  + str.ljust(which_class, 30) +  str(classification[which_class])) \n",
    "                    print(\"\\nSolution path in the decision tree: \" + str(solution_path))             \n",
    "                    print(\"\\nNumber of nodes created: \" + str(root_node.how_many_nodes()))\n",
    "                true_class_label_for_test_sample = self._samples_class_label_dict[test_sample_name]\n",
    "                if evaldebug: \n",
    "                    print(\"%s:   true_class: %s    estimated_class: %s\\n\" % \\\n",
    "                             (test_sample_name, true_class_label_for_test_sample, most_likely_class_label))\n",
    "                confusion_matrix[true_class_label_for_test_sample][most_likely_class_label] += 1    \n",
    "        print(\"\\n\\n       DISPLAYING THE CONFUSION MATRIX FOR THE 10-FOLD CROSS-VALIDATION TEST:\\n\")\n",
    "        matrix_header = \" \" * 30\n",
    "        for class_name in self._class_names:  \n",
    "            matrix_header += '{:^30}'.format(class_name)\n",
    "        print(\"\\n\" + matrix_header + \"\\n\")\n",
    "        for row_class_name in sorted(confusion_matrix.keys()):\n",
    "            row_display = str.rjust(row_class_name, 30)\n",
    "            for col_class_name in sorted(confusion_matrix[row_class_name].keys()):\n",
    "                row_display += '{:^30}'.format(str(confusion_matrix[row_class_name][col_class_name]) )\n",
    "            print(row_display + \"\\n\")\n",
    "        diagonal_sum, off_diagonal_sum = 0,0\n",
    "        for row_class_name in sorted(confusion_matrix.keys()):\n",
    "            for col_class_name in sorted(confusion_matrix[row_class_name].keys()):\n",
    "                if row_class_name == col_class_name:\n",
    "                    diagonal_sum += confusion_matrix[row_class_name][col_class_name]\n",
    "                else:\n",
    "                    off_diagonal_sum += confusion_matrix[row_class_name][col_class_name]\n",
    "        data_quality_index = 100.0 * diagonal_sum / (diagonal_sum + off_diagonal_sum)\n",
    "        print(\"\\nTraining Data Quality Index: %s   (out of a possible maximum of 100)\" % data_quality_index)\n",
    "        if data_quality_index <= 80:\n",
    "            print( '''\\nYour training data does not possess much class discriminatory '''\n",
    "                   '''information.  It could be that the classes are inherently not well '''\n",
    "                   '''separable or that your constructor parameter choices are not appropriate.''')\n",
    "        elif 80 < data_quality_index <= 90:\n",
    "            print( '''\\nYour training data possesses some class discriminatory information '''\n",
    "                   '''but it may not be sufficient for real-world applications.  You might '''\n",
    "                   '''try tweaking the constructor parameters to see if that improves the '''\n",
    "                   '''class discriminations.''')\n",
    "        elif 90 < data_quality_index <= 95:\n",
    "            print( '''\\nYour training data appears to possess good class discriminatory '''\n",
    "                   '''information.  Whether or not it is acceptable would depend on your '''\n",
    "                   '''application.''')\n",
    "        elif 95 < data_quality_index < 98:        \n",
    "            print( '''\\nYour training data is of very high quality.''')\n",
    "        else:\n",
    "            print('''\\nYour training data is excellent.''')\n",
    "\n",
    "\n",
    "#------------------------------  Generate Your Own Numeric Training Data  -----------------------------\n",
    "\n",
    "class TrainingDataGeneratorNumeric(object):\n",
    "    '''\n",
    "    See the example script generate_training_data_numeric.py on how to use this class\n",
    "    for generating your numeric training data.  The training data is generator in\n",
    "    accordance with the specifications you place in a parameter file.\n",
    "    '''\n",
    "    def __init__(self, *args, **kwargs ):\n",
    "        if args:\n",
    "            raise SyntaxError('''TrainingDataGeneratorNumeric can only be called with keyword arguments '''\n",
    "                              '''for the following keywords: output_csv_file, parameter_file, '''\n",
    "                              '''number_of_samples_per_class, and debug''') \n",
    "        allowed_keys = 'output_csv_file','parameter_file','number_of_samples_per_class','debug'\n",
    "        keywords_used = kwargs.keys()\n",
    "        for keyword in keywords_used:\n",
    "            if keyword not in allowed_keys:\n",
    "                raise SyntaxError(\"Wrong keyword used --- check spelling\") \n",
    "        output_csv_file = parameter_file = number_of_samples_per_class = debug = None\n",
    "        if 'output_csv_file' in kwargs : output_csv_file = kwargs.pop('output_csv_file')\n",
    "        if 'parameter_file' in kwargs : parameter_file = kwargs.pop('parameter_file')\n",
    "        if 'number_of_samples_per_class' in kwargs:\n",
    "            number_of_samples_per_class = kwargs.pop('number_of_samples_per_class')\n",
    "        if 'debug' in kwargs:  debug = kwargs.pop('debug')\n",
    "        if output_csv_file:\n",
    "            self._output_csv_file = output_csv_file\n",
    "        else:\n",
    "            raise SyntaxError('''You must specify the name for a csv file for the training data''')\n",
    "        if parameter_file: \n",
    "            self._parameter_file =  parameter_file\n",
    "        else:\n",
    "            raise SyntaxError('''You must specify a parameter file''')\n",
    "        if number_of_samples_per_class:\n",
    "            self._number_of_samples_per_class = number_of_samples_per_class\n",
    "        else:\n",
    "            raise SyntaxError('''You forgot to specify the number of training samples needed per class''')\n",
    "        if debug:\n",
    "            self._debug = debug\n",
    "        else:\n",
    "            self._debug = 0\n",
    "        self._class_names                 = []\n",
    "        self._features_ordered =            []\n",
    "        self._class_names_and_priors      = {}\n",
    "        self._features_with_value_range   = {}\n",
    "        self._classes_and_their_param_values = {}\n",
    "\n",
    "    def read_parameter_file_numeric( self ):\n",
    "        '''\n",
    "        The training data generated by an instance of the class\n",
    "        TrainingDataGeneratorNumeric is based on the specs you place in a parameter\n",
    "        that you supply to the class constructor through a constructor variable\n",
    "        called `parameter_file.  This method is for parsing the parameter file in\n",
    "        order to order to determine the names to be used for the different data\n",
    "        classes, their means, and their variances.\n",
    "        '''\n",
    "        class_names = []\n",
    "        class_names_and_priors = {}\n",
    "        features_with_value_range = {}\n",
    "        classes_and_their_param_values = {}\n",
    "#        regex8 =  '[+-]?\\ *(\\d+(\\.\\d*)?|\\.\\d+)([eE][+-]?\\d+)?';\n",
    "        FILE = open(self._parameter_file)\n",
    "        params = FILE.read()\n",
    "        regex = r'class names: ([\\w\\s+]+)\\W*class priors: ([\\d.\\s+]+)'\n",
    "        classes = re.search(regex, params, re.DOTALL | re.IGNORECASE)\n",
    "        if (classes != None):\n",
    "            class_names = list( filter( None, classes.group(1).strip().split(' ') ) )\n",
    "            class_priors = list( filter( None, classes.group(2).strip().split(' ') ) )\n",
    "        class_names_and_priors = {class_names[i] : float(class_priors[i]) for i in range(len(class_names))}\n",
    "        if self._debug: print(\"\\nClass names and priors: \" + str(class_names_and_priors))\n",
    "        regex = r'feature name: \\w*.*?value range: [\\d\\. -]+'\n",
    "        features = re.findall(regex, params, re.DOTALL | re.IGNORECASE)\n",
    "        regex = r'feature name: (\\w+)\\W*?value range:\\s*([\\d. -]+)'\n",
    "        features_ordered = []\n",
    "        for feature in features:\n",
    "            feature_groups = re.match(regex, feature, re.IGNORECASE)\n",
    "            feature_name = feature_groups.group(1)\n",
    "            features_ordered.append(feature_name)\n",
    "            value_range = feature_groups.group(2).split()\n",
    "            value_range = [float(value_range[0]), float(value_range[2])]\n",
    "            features_with_value_range[feature_name] = value_range\n",
    "        if self._debug: print(\"\\nFeatures and their value ranges: \"+ str(features_with_value_range))\n",
    "        classes_and_their_param_values = {class_names[i] : {} for i in range(len(class_names))}\n",
    "        regex = r'params for class:\\s+\\w*?\\W+?mean:[\\d\\.\\s+]+\\W*?covariance:\\W+?(?:[\\s+\\d.]+\\W+?)+'\n",
    "        class_params = re.findall(regex, params, re.DOTALL | re.IGNORECASE)\n",
    "        regex = r'params for class:\\s+(\\w+)\\W*?mean:\\s*([\\d. -]+)\\W*covariance:\\s*([\\s\\d.]+)'\n",
    "        for class_param in class_params:\n",
    "            class_params_groups = re.match(regex, class_param, re.IGNORECASE)\n",
    "            class_name = class_params_groups.group(1)\n",
    "            class_mean = class_params_groups.group(2).split()\n",
    "            class_mean = list(map(float, class_mean))\n",
    "            classes_and_their_param_values[class_name]['mean'] =  class_mean\n",
    "            vector_size = len(class_mean)\n",
    "            class_param_string = class_params_groups.group(3)\n",
    "            covar_rows = filter(None, \\\n",
    "                  list(map(lambda x: x.strip().split(), class_param_string.splitlines())))\n",
    "            covar_matrix = []\n",
    "            for row in covar_rows:\n",
    "                row = list(map(float, row))\n",
    "                covar_matrix.append(row)\n",
    "            classes_and_their_param_values[class_name]['covariance'] =  covar_matrix\n",
    "        if self._debug: print(\"\\nThe class parameters are: \"+ str(classes_and_their_param_values))\n",
    "        self._class_names                 = class_names\n",
    "        self._class_names_and_priors      = class_names_and_priors\n",
    "        self._features_with_value_range   = features_with_value_range\n",
    "        self._classes_and_their_param_values = classes_and_their_param_values\n",
    "        self._features_ordered = features_ordered\n",
    "\n",
    "    def gen_numeric_training_data_and_write_to_csv(self):\n",
    "        '''\n",
    "        After the parameter file is parsed by the previous method, this method calls\n",
    "        on `numpy.random.multivariate_normal()' to generate the training data\n",
    "        samples. Your training data can be of any number of of dimensions, can have\n",
    "        any mean, and any covariance.\n",
    "        '''\n",
    "        import numpy\n",
    "        import random\n",
    "        samples_for_class = {class_name : [] for class_name in self._class_names}\n",
    "        for class_name in self._classes_and_their_param_values:\n",
    "            mean = self._classes_and_their_param_values[class_name]['mean']\n",
    "            covariance = self._classes_and_their_param_values[class_name]['covariance']\n",
    "            samples = numpy.random.multivariate_normal(mean, covariance, self._number_of_samples_per_class)\n",
    "            samples = [map(float, map(lambda x: \"%.3f\" % x, list_of_samples)) for list_of_samples in samples]\n",
    "            samples_for_class[class_name] = samples\n",
    "        data_records = []     \n",
    "        for class_name in samples_for_class:\n",
    "            for sample_index in range(self._number_of_samples_per_class):\n",
    "                data_record = class_name + \",\" + \\\n",
    "                             ','.join(map(lambda x: str(x), samples_for_class[class_name][sample_index]))\n",
    "                if self._debug: print(\"data record: \" + data_record)\n",
    "                data_records.append(data_record)\n",
    "        random.shuffle(data_records)\n",
    "        sample_records = []\n",
    "        sample_records.append('\"\"' + ',' + 'class_name'  + ',' + ','.join(self._features_ordered) + \"\\n\")\n",
    "        for i in range(len(data_records)):\n",
    "            i1 = i+1\n",
    "            sample_records.append(str(i1) + ',' + data_records[i] +\"\\n\")\n",
    "        FILE = open(self._output_csv_file, 'w') \n",
    "        list(map(FILE.write, sample_records))\n",
    "        FILE.close()    \n",
    "\n",
    "#------------------------  Generate Your Own Symbolic Training Data  --------------------------------\n",
    "\n",
    "class TrainingDataGeneratorSymbolic(object):\n",
    "    '''\n",
    "    See the sample script generate_training_data_symbolic.py for how to use this\n",
    "    class for generating symbolic training data.  The data is generated according to\n",
    "    the specifications you place in a parameter file.\n",
    "    '''\n",
    "    def __init__(self, *args, **kwargs ):\n",
    "        if args:\n",
    "            raise SyntaxError('''TrainingDataGeneratorSymbolic can only be called with keyword arguments for the '''\n",
    "                             '''following keywords: output_datafile, parameter_file, number_of_training_samples, '''\n",
    "                             '''write_to_file, debug1, and debug2''') \n",
    "        allowed_keys = 'output_datafile','parameter_file','number_of_training_samples', 'write_to_file','debug1','debug2'\n",
    "        keywords_used = kwargs.keys()\n",
    "        for keyword in keywords_used:\n",
    "            if keyword not in allowed_keys:\n",
    "                raise SyntaxError(\"Wrong keyword used --- check spelling\") \n",
    "        output_datafile = parameter_file = number_of_training_samples = None\n",
    "        write_to_file = debug1 = debug2 = None\n",
    "        if 'output_datafile' in kwargs: output_datafile = kwargs.pop('output_datafile')\n",
    "        if 'parameter_file' in kwargs:  parameter_file = kwargs.pop('parameter_file')\n",
    "        if 'number_of_training_samples' in kwargs: \n",
    "            number_of_training_samples = kwargs.pop('number_of_training_samples')\n",
    "        if 'write_to_file' in kwargs : write_to_file = kwargs.pop('write_to_file')\n",
    "        if 'debug1' in kwargs:  debug1 = kwargs.pop('debug1')\n",
    "        if 'debug2' in kwargs:  debug2 = kwargs.pop('debug2')\n",
    "        if output_datafile:\n",
    "            self._output_datafile = output_datafile\n",
    "        else:\n",
    "            raise SyntaxError('''You must specify an output datafile''')\n",
    "        if parameter_file: \n",
    "            self._parameter_file =  parameter_file\n",
    "        else:\n",
    "            raise SyntaxError('''You must specify a parameter file''')\n",
    "        if number_of_training_samples:\n",
    "            self._number_of_training_samples = number_of_training_samples\n",
    "        else:\n",
    "            raise SyntaxError('''You forgot to specify the number of training samples needed''')\n",
    "        if write_to_file:\n",
    "            self._write_to_file = write_to_file\n",
    "        else:\n",
    "            self._write_to_file = 0          \n",
    "        if debug1:\n",
    "            self._debug1 = debug1\n",
    "        else:\n",
    "            self._debug1 = 0\n",
    "        if debug2:\n",
    "            self._debug2 = debug2\n",
    "        else:\n",
    "            self._debug2 = 0\n",
    "        self._training_sample_records     = {}\n",
    "        self._features_and_values_dict    = {}\n",
    "        self._bias_dict                   = {}\n",
    "        self._class_names                 = []\n",
    "        self._class_priors                = []\n",
    "\n",
    "\n",
    "    def read_parameter_file_symbolic( self ):\n",
    "        '''\n",
    "        Read a parameter file for generating symbolic training data. See the script\n",
    "        generate_training_data_symbolic.py in the Examples directory for how to pass\n",
    "        the name of the parameter file to the constructor of the\n",
    "        TrainingDataGeneratorSymbolic class.\n",
    "        '''\n",
    "        debug1 = self._debug1\n",
    "        debug2 = self._debug2\n",
    "        write_to_file = self._write_to_file\n",
    "        number_of_training_samples = self._number_of_training_samples\n",
    "        input_parameter_file = self._parameter_file\n",
    "        all_params = []\n",
    "        param_string = ''\n",
    "        FILE = open(input_parameter_file, 'r')\n",
    "        all_params = FILE.read()\n",
    "        all_params = re.split(r'\\n', all_params)\n",
    "        FILE.close()\n",
    "        pattern = r'^(?![ ]*#)'\n",
    "        try:\n",
    "            regex = re.compile( pattern )\n",
    "        except:\n",
    "            print(\"error in your pattern\")\n",
    "            raise\n",
    "        all_params = list( filter( regex.search, all_params ) )\n",
    "        all_params = list( filter( None, all_params ) )\n",
    "        all_params = [x.rstrip('\\n') for x in all_params]\n",
    "        param_string = ' '.join( all_params )\n",
    "        pattern = '^\\s*class names:(.*?)\\s*class priors:(.*?)(feature: .*)'\n",
    "        m = re.search( pattern, param_string )\n",
    "        rest_params = m.group(3)\n",
    "        self._class_names = list( filter(None, re.split(r'\\s+', m.group(1))) )\n",
    "        self._class_priors = list( filter(None, re.split(r'\\s+', m.group(2))) )\n",
    "        pattern = r'(feature:.*?) (bias:.*)'\n",
    "        m = re.search( pattern, rest_params  )\n",
    "        feature_string = m.group(1)\n",
    "        bias_string = m.group(2)\n",
    "        features_and_values_dict = {}\n",
    "        features = list( filter( None, re.split( r'(feature[:])', feature_string ) ) )\n",
    "        for item in features:\n",
    "            if re.match(r'feature', item): continue\n",
    "            splits = list( filter(None, re.split(r' ', item)) )\n",
    "            for i in range(0, len(splits)):\n",
    "                if i == 0: features_and_values_dict[splits[0]] = []\n",
    "                else:\n",
    "                    if re.match( r'values', splits[i] ): continue\n",
    "                    features_and_values_dict[splits[0]].append( splits[i] )\n",
    "        self._features_and_values_dict = features_and_values_dict\n",
    "        bias_dict = {}\n",
    "        biases = list( filter(None, re.split(r'(bias[:]\\s*class[:])', bias_string )) )\n",
    "        for item in biases:\n",
    "            if re.match(r'bias', item): continue\n",
    "            splits = list( filter(None, re.split(r' ', item)) )\n",
    "            feature_name = ''\n",
    "            for i in range(0, len(splits)):\n",
    "                if i == 0:\n",
    "                    bias_dict[splits[0]] = {}\n",
    "                elif ( re.search( r'(^.+)[:]$', splits[i] ) ):\n",
    "                    m = re.search(  r'(^.+)[:]$', splits[i] )\n",
    "                    feature_name = m.group(1)\n",
    "                    bias_dict[splits[0]][feature_name] = []\n",
    "                else:\n",
    "                    if not feature_name: continue\n",
    "                    bias_dict[splits[0]][feature_name].append( splits[i] )\n",
    "        self._bias_dict = bias_dict\n",
    "        if self._debug1:\n",
    "            print(\"\\n\\n\") \n",
    "            print(\"Class names: \" + str(self._class_names))\n",
    "            print(\"\\n\") \n",
    "            num_of_classes = len(self._class_names)\n",
    "            print(\"Number of classes: \" + str(num_of_classes))\n",
    "            print(\"\\n\")\n",
    "            print(\"Class priors: \" + str(self._class_priors))\n",
    "            print(\"\\n\\n\")\n",
    "            print(\"Here are the features and their possible values\")\n",
    "            print(\"\\n\")\n",
    "            items = self._features_and_values_dict.items()\n",
    "            for item in items:\n",
    "                print(item[0] + \" ===> \" + str(item[1]))\n",
    "            print(\"\\n\")\n",
    "            print(\"Here is the biasing for each class:\")\n",
    "            print(\"\\n\")          \n",
    "            items = self._bias_dict.items()\n",
    "            for item in items:\n",
    "                print(\"\\n\")\n",
    "                print(item[0])\n",
    "                items2 = list( item[1].items() )\n",
    "                for i in range(0, len(items2)):\n",
    "                    print( items2[i])\n",
    "\n",
    "    def gen_symbolic_training_data( self ):\n",
    "        '''\n",
    "        This method generates training data according to the specifications\n",
    "        placed in a parameter file that is read by the previous method.\n",
    "        '''\n",
    "        class_names = self._class_names\n",
    "        class_priors = self._class_priors\n",
    "        training_sample_records = {}\n",
    "        features_and_values_dict = self._features_and_values_dict\n",
    "        bias_dict  = self._bias_dict\n",
    "        how_many_training_samples = self._number_of_training_samples\n",
    "        class_priors_to_unit_interval_map = {}\n",
    "        accumulated_interval = 0\n",
    "        for i in range(0, len(class_names)):\n",
    "            class_priors_to_unit_interval_map[class_names[i]] = \\\n",
    "            (accumulated_interval, accumulated_interval+float(class_priors[i]))\n",
    "            accumulated_interval += float(class_priors[i])\n",
    "        if self._debug1:\n",
    "            print(\"Mapping of class priors to unit interval:\")\n",
    "            print(\"\\n\")\n",
    "            items = class_priors_to_unit_interval_map.items()\n",
    "            for item in items:\n",
    "                print(item[0] + \" ===> \" + str(item[1]))\n",
    "        class_and_feature_based_value_priors_to_unit_interval_map = {}\n",
    "        for class_name  in class_names:\n",
    "            class_and_feature_based_value_priors_to_unit_interval_map[class_name] = {}\n",
    "            for feature in features_and_values_dict:\n",
    "                class_and_feature_based_value_priors_to_unit_interval_map[class_name][feature] = {}\n",
    "        for class_name  in class_names:\n",
    "            for feature in features_and_values_dict:\n",
    "                values = features_and_values_dict[feature]\n",
    "                if len(bias_dict[class_name][feature]) > 0:\n",
    "                    bias_string = bias_dict[class_name][feature][0]\n",
    "                else:\n",
    "                    no_bias = 1.0 / len(values)\n",
    "                    bias_string = values[0] +  \"=\" + str(no_bias)\n",
    "                value_priors_to_unit_interval_map = {}\n",
    "                splits = list( filter( None, re.split(r'\\s*=\\s*', bias_string) ) )\n",
    "                chosen_for_bias_value = splits[0]\n",
    "                chosen_bias = splits[1]\n",
    "                remaining_bias = 1 - float(chosen_bias)\n",
    "                remaining_portion_bias = remaining_bias / (len(values) -1)\n",
    "                accumulated = 0;\n",
    "                for i in range(0, len(values)):\n",
    "                    if (values[i] == chosen_for_bias_value):\n",
    "                        value_priors_to_unit_interval_map[values[i]] = \\\n",
    "                          [accumulated, accumulated + float(chosen_bias)]\n",
    "                        accumulated += float(chosen_bias)\n",
    "                    else:\n",
    "                        value_priors_to_unit_interval_map[values[i]] = \\\n",
    "                          [accumulated, accumulated + remaining_portion_bias]\n",
    "                        accumulated += remaining_portion_bias\n",
    "                class_and_feature_based_value_priors_to_unit_interval_map[class_name][feature] = \\\n",
    "                                                                   value_priors_to_unit_interval_map\n",
    "                if self._debug2:\n",
    "                    print(\"\\n\")\n",
    "                    print( \"For class \" + class_name + \\\n",
    "                       \": Mapping feature value priors for feature '\" + \\\n",
    "                       feature + \"' to unit interval: \")\n",
    "                    print(\"\\n\")\n",
    "                    items = value_priors_to_unit_interval_map.items()\n",
    "                    for item in items:\n",
    "                        print(\"    \" + item[0] + \" ===> \" + str(item[1]))\n",
    "        ele_index = 0\n",
    "        while (ele_index < how_many_training_samples):\n",
    "            sample_name = \"sample\" + \"_\" + str(ele_index)\n",
    "            training_sample_records[sample_name] = []\n",
    "            # Generate class label for this training sample:                \n",
    "            import random\n",
    "            ran = random.Random()\n",
    "            roll_the_dice  = ran.randint(0,1000) / 1000.0\n",
    "            class_label = ''\n",
    "            for class_name  in class_priors_to_unit_interval_map:\n",
    "                v = class_priors_to_unit_interval_map[class_name]\n",
    "                if ( (roll_the_dice >= v[0]) and (roll_the_dice <= v[1]) ):\n",
    "                    training_sample_records[sample_name].append( \n",
    "                                             \"class=\" + class_name )\n",
    "                    class_label = class_name\n",
    "                    break\n",
    "            for feature in sorted(list(features_and_values_dict.keys())):\n",
    "                roll_the_dice  = ran.randint(0,1000) / 1000.0\n",
    "                value_label = ''\n",
    "                value_priors_to_unit_interval_map = \\\n",
    "                  class_and_feature_based_value_priors_to_unit_interval_map[class_label][feature]\n",
    "                for value_name in value_priors_to_unit_interval_map:\n",
    "                    v = value_priors_to_unit_interval_map[value_name]\n",
    "                    if ( (roll_the_dice >= v[0]) and (roll_the_dice <= v[1]) ):\n",
    "                        training_sample_records[sample_name].append( \\\n",
    "                                            feature + \"=\" + value_name )\n",
    "                        value_label = value_name;\n",
    "                        break\n",
    "            ele_index += 1\n",
    "        self._training_sample_records = training_sample_records\n",
    "        if self._debug2:\n",
    "            print(\"\\n\\n\")\n",
    "            print(\"TERMINAL DISPLAY OF TRAINING RECORDS:\")\n",
    "            print(\"\\n\\n\")\n",
    "            sample_names = training_sample_records.keys()\n",
    "            sample_names = sorted( sample_names, key=lambda x: int(x.lstrip('sample_')) )\n",
    "            for sample_name in sample_names:\n",
    "                print(sample_name + \" = \" + str(training_sample_records[sample_name]))\n",
    "\n",
    "    def write_training_data_to_file( self ):\n",
    "        features_and_values_dict = self._features_and_values_dict\n",
    "        class_names = self._class_names\n",
    "        output_file = self._output_datafile\n",
    "        training_sample_records = self._training_sample_records\n",
    "        FILE = open(self._output_datafile, 'w') \n",
    "        features = list( features_and_values_dict.keys() )\n",
    "        features.sort()\n",
    "        title_string = ',class'\n",
    "        for feature_name in features:\n",
    "            title_string += ',' + str(feature_name)\n",
    "        FILE.write(title_string + \"\\n\")\n",
    "        sample_names = list( training_sample_records.keys() )\n",
    "        sample_names = sorted( sample_names, key=lambda x: int(x.lstrip('sample_')) )\n",
    "        record_string = ''\n",
    "        for sample_name in sample_names:\n",
    "            sample_name_string =  str(sample_index(sample_name))\n",
    "            record_string += sample_name_string + ','\n",
    "            record = training_sample_records[sample_name]\n",
    "            item_parts_dict = {}\n",
    "            for item in record:\n",
    "                splits = list( filter(None, re.split(r'=', item)) )\n",
    "                item_parts_dict[splits[0]] = splits[1]\n",
    "            record_string += item_parts_dict[\"class\"]\n",
    "            del item_parts_dict[\"class\"]\n",
    "            kees = list(item_parts_dict.keys())\n",
    "            kees.sort()\n",
    "            for kee in kees:\n",
    "                record_string += \",\" + item_parts_dict[kee]\n",
    "            FILE.write(record_string + \"\\n\")\n",
    "            record_string = ''\n",
    "        FILE.close()\n",
    "\n",
    "\n",
    "#----------------------------------  Class DTIntrospection   ----------------------------------\n",
    "\n",
    "class DTIntrospection(object):\n",
    "    '''Instances constructed from this class can provide explanations for the\n",
    "    classification decisions at the nodes of a decision tree.  \n",
    "\n",
    "    When used in the interactive mode, the decision-tree introspection made possible\n",
    "    by this class provides answers to the following three questions: (1) List of the\n",
    "    training samples that fall in the portion of the feature space that corresponds\n",
    "    to a node of the decision tree; (2) The probabilities associated with the last\n",
    "    feature test that led to the node; and (3) The class probabilities predicated on\n",
    "    just the last feature test on the path to that node.\n",
    "\n",
    "    CAVEAT: It is possible for a node to exist even when there are no training\n",
    "    samples in the portion of the feature space that corresponds to the node.  That\n",
    "    is because a decision tree is based on the probability densities estimated from\n",
    "    the training data. When training data is non-uniformly distributed, it is\n",
    "    possible for the probability associated with a point in the feature space to be\n",
    "    non-zero even when there are no training samples at or in the vicinity of that\n",
    "    point.\n",
    "\n",
    "    For a node to exist even where there are no training samples in the portion of\n",
    "    the feature space that belongs to the node is an indication of the generalization\n",
    "    ability of decision-tree based classification.\n",
    "\n",
    "    When used in a non-interactive mode, an instance of this class can be used to\n",
    "    create a tabular display that shows what training samples belong directly to the\n",
    "    portion of the feature space that corresponds to each node of the decision tree.\n",
    "    An instance of this class can also construct a tabular display that shows how the\n",
    "    influence of each training sample propagates in the decision tree.  For each\n",
    "    training sample, this display first shows the list of nodes that came into\n",
    "    existence through feature test(s) that used the data provided by that sample.\n",
    "    This list for each training sample is followed by a subtree of the nodes that owe\n",
    "    their existence indirectly to the training sample. A training sample influences a\n",
    "    node indirectly if the node is a descendant of another node that is affected\n",
    "    directly by the training sample.\n",
    "\n",
    "    '''\n",
    "    def __init__(self, dt):\n",
    "        if isinstance(dt, DecisionTree) is False:\n",
    "            raise TypeError(\"The argument supplied to the DTIntrospector constructor must be of type DecisionTree\")\n",
    "        self._dt = dt\n",
    "        self._root_dtnode =  dt._root_node\n",
    "        self._samples_at_nodes_dict = {}\n",
    "        self._branch_features_to_nodes_dict = {}\n",
    "        self._sample_to_node_mapping_direct_dict = {}\n",
    "        self._node_serial_num_to_node_dict = {} \n",
    "        self._awareness_raising_msg_shown = 0\n",
    "        self._debug = 0\n",
    "\n",
    "    def initialize(self):\n",
    "        if self._root_dtnode is None:\n",
    "            raise SyntaxError(\"You must first construct the decision tree before using the DT Introspection class.\")\n",
    "        root_node = self._root_dtnode\n",
    "        self.recursive_descent(root_node)\n",
    "\n",
    "    def recursive_descent(self, node):\n",
    "        node_serial_number = node.get_serial_num()\n",
    "        self._node_serial_num_to_node_dict[node_serial_number] = node\n",
    "        branch_features_and_values_or_thresholds = node.get_branch_features_and_values_or_thresholds()\n",
    "        if self._debug:\n",
    "            print(\"\\nat node \" + str(node_serial_number) + \":  the branch features and values are: \" + str(branch_features_and_values_or_thresholds))\n",
    "        self._branch_features_to_nodes_dict[node_serial_number] = branch_features_and_values_or_thresholds\n",
    "        samples_at_node = None\n",
    "        for item in branch_features_and_values_or_thresholds:\n",
    "            samples_for_feature_value_combo = self.get_samples_for_feature_value_combo(item)\n",
    "            samples_at_node = samples_for_feature_value_combo if samples_at_node is None else \\\n",
    "                        [sample for sample in samples_at_node if sample in samples_for_feature_value_combo]\n",
    "        if (samples_at_node is not None) and (len(samples_at_node) > 0): \n",
    "            samples_at_node = sorted(samples_at_node, key = lambda x: sample_index(x))\n",
    "        if self._debug:\n",
    "            print(\"Node: \" + str(node_serial_number) + \" the samples are: \" + str(samples_at_node))\n",
    "        self._samples_at_nodes_dict[node_serial_number] = samples_at_node\n",
    "        if samples_at_node is not None:\n",
    "            for sample in samples_at_node:\n",
    "                if sample not in self._sample_to_node_mapping_direct_dict:\n",
    "                    self._sample_to_node_mapping_direct_dict[sample] = [node_serial_number] \n",
    "                else:\n",
    "                    self._sample_to_node_mapping_direct_dict[sample].append(node_serial_number)\n",
    "        children = node.get_children()\n",
    "        list(map(lambda x: self.recursive_descent(x), children))\n",
    "\n",
    "    def display_training_samples_at_all_nodes_direct_influence_only(self):\n",
    "        if self._root_dtnode is None:\n",
    "            raise SyntaxError(\"You must first construct the decision tree before using the DT Introspection class.\")\n",
    "        root_node = self._root_dtnode\n",
    "        self.recursive_descent_for_showing_samples_at_a_node(root_node)\n",
    "\n",
    "    def recursive_descent_for_showing_samples_at_a_node(self, node):\n",
    "        node_serial_number = node.get_serial_num()\n",
    "        branch_features_and_values_or_thresholds = node.get_branch_features_and_values_or_thresholds()\n",
    "        if node_serial_number in self._samples_at_nodes_dict:\n",
    "            if self._debug:\n",
    "                print(\"\\nat node \" + str(node_serial_number) + \":  the branch features and values are: \" + str(branch_features_and_values_or_thresholds))\n",
    "            print(\"Node \" + str(node_serial_number) + \": the samples are: \" + str(self._samples_at_nodes_dict[node_serial_number]))\n",
    "        children = node.get_children()\n",
    "        list(map(lambda x: self.recursive_descent_for_showing_samples_at_a_node(x), children))\n",
    "\n",
    "    def display_training_samples_to_nodes_influence_propagation(self):\n",
    "        for sample in sorted(self._dt._training_data_dict, key = lambda x: sample_index(x)):\n",
    "            if sample in self._sample_to_node_mapping_direct_dict:\n",
    "                nodes_directly_affected = self._sample_to_node_mapping_direct_dict[sample]\n",
    "                print(\"\\n\" + sample + \":\\n\" + \"   nodes affected directly: \" + str(nodes_directly_affected))\n",
    "                print(\"   nodes affected through probabilistic generalization:\")\n",
    "                list(map(lambda x: self.recursive_descent_for_sample_to_node_influence(x, nodes_directly_affected, \"    \"), nodes_directly_affected))\n",
    "\n",
    "    def recursive_descent_for_sample_to_node_influence(self, node_serial_num, nodes_already_accounted_for, offset ):\n",
    "        offset += \"    \"  \n",
    "        node =  self._node_serial_num_to_node_dict[node_serial_num]\n",
    "        children = [x.get_serial_num() for x in node.get_children()]\n",
    "        children_affected = [child for child in children if child not in nodes_already_accounted_for]\n",
    "        if len(children_affected) > 0:\n",
    "            print(offset + str(node_serial_num) + \"=> \" + str(children_affected))\n",
    "        list(map(lambda x: self.recursive_descent_for_sample_to_node_influence(x, children_affected, offset), children_affected))\n",
    "\n",
    "    def get_samples_for_feature_value_combo(self, feature_value_combo):\n",
    "        feature,op,value = self.extract_feature_op_val(feature_value_combo)\n",
    "        samples = []\n",
    "        if op == '=':\n",
    "            samples = [sample for sample in self._dt._training_data_dict if feature_value_combo in \\\n",
    "                                                       self._dt._training_data_dict[sample]]\n",
    "        elif op == '<':      \n",
    "            for sample in self._dt._training_data_dict:\n",
    "                features_and_values = self._dt._training_data_dict[sample]            \n",
    "                for item in features_and_values:\n",
    "                    feature_data,op_data,val_data = self.extract_feature_op_val(item)\n",
    "                    value = convert(value)\n",
    "                    val_data = convert(val_data)\n",
    "                    if isinstance(val_data, (int,float)) and (feature == feature_data) and (val_data <= value):\n",
    "                        samples.append(sample)\n",
    "                        break\n",
    "        elif op == '>':      \n",
    "            for sample in self._dt._training_data_dict:\n",
    "                features_and_values = self._dt._training_data_dict[sample]            \n",
    "                for item in features_and_values:\n",
    "                    feature_data,op_data,val_data = self.extract_feature_op_val(item)\n",
    "                    value = convert(value)\n",
    "                    val_data = convert(val_data)\n",
    "                    if isinstance(val_data, (int,float)) and (feature == feature_data) and (val_data > value):\n",
    "                        samples.append(sample)\n",
    "                        break\n",
    "        else:\n",
    "            raise SyntaxError(\"Something is wrong with the feature-value syntax\")\n",
    "        return samples\n",
    "\n",
    "    def extract_feature_op_val(self, feature_value_combo):\n",
    "        pattern1 = r'(.+)=(.+)'\n",
    "        pattern2 = r'(.+)<(.+)'\n",
    "        pattern3 = r'(.+)>(.+)'\n",
    "        feature = value = op = None\n",
    "        if re.search(pattern2, feature_value_combo):\n",
    "            m = re.search(pattern2, feature_value_combo)\n",
    "            feature,op,value = m.group(1),'<',m.group(2)\n",
    "        elif re.search(pattern3, feature_value_combo): \n",
    "            m = re.search(pattern3, feature_value_combo)\n",
    "            feature,op,value = m.group(1),'>',m.group(2)\n",
    "        else:\n",
    "            m = re.search(pattern1, feature_value_combo)\n",
    "            feature,op,value = m.group(1),'=',m.group(2)\n",
    "        return (feature,op,value) \n",
    "\n",
    "    def explain_classifications_at_multiple_nodes_interactively(self):\n",
    "        if not self._samples_at_nodes_dict:\n",
    "            raise SyntaxError('''You called explain_classifications_at_multiple_nodes_interactively() without '''\n",
    "                              '''first initializing the DTIntrospection instance in your code. Aborting.''')\n",
    "        print(\"\\n\\nIn order for the decision tree to introspect\\n\")\n",
    "        msg = '''DO YOU ACCEPT the fact that, in general, a region of the feature space\n",
    "                 that corresponds to a node may have NON-ZERO probabilities associated\n",
    "                 with it even when there are NO training data points in that region?\n",
    "                 Enter 'y' for yes or any other character for no: '''\n",
    "        ans = None\n",
    "        if sys.version_info[0] == 3:\n",
    "            ans = input(msg)\n",
    "        else:\n",
    "            ans = raw_input(msg)\n",
    "        ans = ans.strip()\n",
    "        if (ans != 'y') and (ans != 'yes'):\n",
    "            raise Exception(\"\\n  Since you answered 'no' to a very real theoretical possibility, no explanations possible for the classification decisions in the decision tree. Aborting\")\n",
    "        self._awareness_raising_msg_shown = 1\n",
    "        while True:\n",
    "            node_id = None\n",
    "            while True:\n",
    "                ans = None\n",
    "                if sys.version_info[0] == 3:\n",
    "                    ans = input(\"\\nEnter the integer ID of a node: \")\n",
    "                else:\n",
    "                    ans = raw_input(\"\\nEnter the integer ID of a node: \")\n",
    "                ans = ans.strip()\n",
    "                if ans == 'exit': return\n",
    "                try:\n",
    "                    ans = int(ans)\n",
    "                except ValueError:\n",
    "                    print(\"\\nWhat you entered does not look like an integer ID for a node. Aborting!\")\n",
    "                    raise\n",
    "                if ans in self._samples_at_nodes_dict:\n",
    "                    node_id = ans\n",
    "                    break;\n",
    "                else:\n",
    "                   print(\"\\nYour answer must be an integer ID of a node. Try again or enter 'exit'.\")\n",
    "            self.explain_classification_at_one_node(node_id)   \n",
    "\n",
    "    def explain_classification_at_one_node(self, node_id):\n",
    "        if not self._samples_at_nodes_dict:\n",
    "            raise SyntaxError('''You called explain_classification_at_one_node() without first initializing '''\n",
    "                              '''the DTIntrospection instance in your code. Aborting.\"''')\n",
    "        if node_id not in self._samples_at_nodes_dict:\n",
    "            print(\"Node %d is not a node in the tree\" % node_id)\n",
    "            return\n",
    "        if node_id == 0:\n",
    "            print(\"Nothing useful to be explained at the root node\")\n",
    "            return\n",
    "        if not self._awareness_raising_msg_shown: \n",
    "            print(\"\\n\\nIn order for the decision tree to introspect at Node %d: \\n\" % node_id)\n",
    "            msg = \"  DO YOU ACCEPT the fact that, in general, a region of the feature space\\n\" + \\\n",
    "                  \"  that corresponds to a DT node may have NON-ZERO probabilities associated\\n\" + \\\n",
    "                  \"  with it even when there are NO training data points in that region?\\n\" + \\\n",
    "                  \"\\nEnter 'y' for yes or any other character for no:  \"\n",
    "            ans = None\n",
    "            if sys.version_info[0] == 3:\n",
    "                ans = input(msg)\n",
    "            else:\n",
    "                ans = raw_input(msg)\n",
    "            ans = ans.strip()\n",
    "            if (ans != 'y') and (ans != 'yes'):\n",
    "                raise Exception(\"\\n  Since you answered 'no' to a very real theoretical possibility, no explanations possible for the classification decision at node %d\" % node_id)\n",
    "        samples_at_node = self._samples_at_nodes_dict[node_id]\n",
    "        branch_features_to_node = self._branch_features_to_nodes_dict[node_id]\n",
    "        class_names = self._root_dtnode.get_class_names()\n",
    "        class_probabilities = self._root_dtnode.get_class_probabilities()\n",
    "        feature,op,value = self.extract_feature_op_val( branch_features_to_node[-1] )\n",
    "        if len(samples_at_node) > 0:\n",
    "            msg2 = \"\\n    Samples in the portion of the feature space assigned to Node %d: %s\\n\" % (node_id,  str(samples_at_node))\n",
    "        else:\n",
    "            msg2 = \"\\n\\n    There are NO training data samples directly in the region of the feature space assigned to node %d.\\n\" % node_id\n",
    "        msg2 += \"\\n    Features tests on the branch to node %d: %s\\n\" % (node_id, str(branch_features_to_node))\n",
    "        msg2 += \"\\n    Would you like to see the probability associated with the last feature test on the branch leading to Node %d\\n?\" % node_id\n",
    "        msg2 += \"\\n\\n    Enter 'y' if yes and `n' if no:  \"\n",
    "        ans = None\n",
    "        if sys.version_info[0] == 3:\n",
    "            ans = input(msg2)\n",
    "        else:\n",
    "            ans = raw_input(msg2)\n",
    "        ans = ans.strip()\n",
    "        if (ans == 'y') or (ans == 'yes'):\n",
    "            sequence = [ branch_features_to_node[-1] ]\n",
    "            prob = self._dt.probability_of_a_sequence_of_features_and_values_or_thresholds(sequence) \n",
    "            print(\"\\n    Probability of %s is: %s\" % (str(sequence), str(prob))) \n",
    "        msg3 = \"\\n    Using Bayes rule, would you like to see the class probabilities predicated on just the last feature test on the branch leading to Node %d\\n?\" % node_id\n",
    "        msg3 += \"\\n\\n    Enter 'y' if yes and `n' if no:  \"\n",
    "        ans = None\n",
    "        if sys.version_info[0] == 3:\n",
    "            ans = input(msg3)\n",
    "        else:\n",
    "            ans = raw_input(msg3)\n",
    "        ans = ans.strip()\n",
    "        if (ans == 'y') or (ans == 'yes'):\n",
    "            sequence = [ branch_features_to_node[-1] ]\n",
    "            for cls in class_names:\n",
    "                prob = self._dt.probability_of_a_class_given_sequence_of_features_and_values_or_thresholds(cls, sequence)\n",
    "                print(\"\\n    Probability of class %s given just one feature test %s is: %s\" % (cls, str(sequence), str(prob)))\n",
    "        else:\n",
    "            print(\"goodbye\")\n",
    "        print(\"\\nFinished supplying information on Node %d\\n\\n\" % node_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    dt = DecisionTree( training_datafile = \"training_symbolic.csv\",  \n",
    "                        csv_class_column_index = 1,\n",
    "                        csv_columns_for_features = [2,3,4,5],\n",
    "                        max_depth_desired = 5,\n",
    "                        entropy_threshold = 0.1,\n",
    "                     )\n",
    "    dt.get_training_data()\n",
    "\n",
    "    dt.show_training_data()\n",
    "\n",
    "    prob = dt.prior_probability_for_class( 'class=benign' )\n",
    "    print(\"prior for benign: \", prob)\n",
    "    prob = dt.prior_probability_for_class( 'class=malignant' )\n",
    "    print(\"prior for malignant: \", prob)\n",
    "\n",
    "    prob = dt.probability_of_feature_value( 'smoking', 'heavy')\n",
    "    print(prob)\n",
    "\n",
    "    dt.determine_data_condition()\n",
    "\n",
    "    root_node = dt.construct_decision_tree_classifier()\n",
    "    root_node.display_decision_tree(\"   \")\n",
    "\n",
    "    test_sample = ['exercising=never', 'smoking=heavy', 'fatIntake=heavy', 'videoAddiction=heavy']\n",
    "    classification = dt.classify(root_node, test_sample)\n",
    "    print(\"Classification: \" + str(classification))\n",
    "\n",
    "    test_sample = ['videoAddiction=none', 'exercising=occasionally', 'smoking=never', 'fatIntake=medium']\n",
    "    classification = dt.classify(root_node, test_sample)\n",
    "    print(\"Classification: \" + str(classification))\n",
    "\n",
    "    print(\"Number of nodes created: \" + str(root_node.how_many_nodes()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_index(sample_name):\n",
    "    '''\n",
    "    When the training data is read from a CSV file, we assume that the first column\n",
    "    of each data record contains a unique integer identifier for the record in that\n",
    "    row. This training data is stored in a dictionary whose keys are the prefix\n",
    "    'sample_' followed by the identifying integers. The purpose of this function is to\n",
    "    return the identifying integer associated with a data record.\n",
    "    '''\n",
    "    m = re.search('_(.+)$', sample_name)\n",
    "    return int(m.group(1))\n",
    "\n",
    "def deep_copy_array(array_in):\n",
    "    '''\n",
    "    Meant only for an array of scalars (no nesting):\n",
    "    '''\n",
    "    array_out = []\n",
    "    for i in range(len(array_in)):\n",
    "        array_out.append( array_in[i] )\n",
    "    return array_out\n",
    "\n",
    "def minimum(arr):\n",
    "    '''\n",
    "    Returns simultaneously the minimum value and its positional index in an\n",
    "    array. [Could also have used min() and index() defined for Python's\n",
    "    sequence types.]\n",
    "    '''\n",
    "    minval,index = None,None\n",
    "    for i in range(0, len(arr)):  \n",
    "        if minval is None or arr[i] < minval:\n",
    "            index = i\n",
    "            minval = arr[i]\n",
    "    return minval,index\n",
    "\n",
    "def convert(value):\n",
    "    try:\n",
    "        answer = float(value)\n",
    "        return answer\n",
    "    except:\n",
    "        return value\n",
    "\n",
    "def closest_sampling_point(value, arr):\n",
    "    if value == 'NA': return None                # in version 3.3.0\n",
    "    compare = [abs(x - value) for x in arr]\n",
    "    minval,index = minimum(compare)\n",
    "    return arr[index]\n",
    "\n",
    "def cleanup_csv(line):\n",
    "    line = line.translate(bytes.maketrans(b\":?/()[]{}'\",b\"          \")) \\\n",
    "           if sys.version_info[0] == 3 else line.translate(string.maketrans(\":?/()[]{}'\",\"          \"))\n",
    "    double_quoted = re.findall(r'\"[^\\\"]+\"', line[line.find(',') : ])\n",
    "    for item in double_quoted:\n",
    "        clean = re.sub(r',', r'', item[1:-1].strip())\n",
    "        parts = re.split(r'\\s+', clean.strip())\n",
    "        line = str.replace(line, item, '_'.join(parts))\n",
    "    white_spaced = re.findall(r',(\\s*[^,]+)(?=,|$)', line)\n",
    "    for item in white_spaced:\n",
    "        litem = item\n",
    "        litem = re.sub(r'\\s+', '_', litem)\n",
    "        litem = re.sub(r'^\\s*_|_\\s*$', '', litem) \n",
    "        line = str.replace(line, \",\" + item, \",\" + litem) if line.endswith(item) else str.replace(line, \",\" + item + \",\", \",\" + litem + \",\") \n",
    "    fields = re.split(r',', line)\n",
    "    newfields = []\n",
    "    for field in fields:\n",
    "        newfield = field.strip()\n",
    "        if newfield == '':\n",
    "            newfields.append('NA')\n",
    "        else:\n",
    "            newfields.append(newfield)\n",
    "    line = ','.join(newfields)\n",
    "    return line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_datafile = \"stage3cancer.csv\"\n",
    "dt = DecisionTree( training_datafile = training_datafile,\n",
    "                        csv_class_column_index = 8,\n",
    "                        csv_columns_for_features = [1,2,3,4,5,6,7],\n",
    "                        entropy_threshold = 0.01,\n",
    "                        max_depth_desired = 8,\n",
    "                        symbolic_to_numeric_cardinality_threshold = 10,\n",
    "                        csv_cleanup_needed = 1,\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt.get_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt.show_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob = dt.prior_probability_for_class( 'ploidy=diploid' )\n",
    "print(\"prior for diploid: \", prob)\n",
    "prob = dt.prior_probability_for_class( 'ploidy=tetraploid' )\n",
    "print(\"prior for tetraploid: \", prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
